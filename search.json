[
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "inference",
    "section": "",
    "text": "source\n\nYOLOXInferenceWrapper\n\n YOLOXInferenceWrapper (model:torch.nn.modules.module.Module,\n                        normalize_mean:torch.Tensor=tensor([[[[0.]],\n                        [[0.]],           [[0.]]]]),\n                        normalize_std:torch.Tensor=tensor([[[[1.]],\n                        [[1.]],           [[1.]]]]),\n                        strides:Optional[List[int]]=[8, 16, 32],\n                        scale_inp:bool=False, channels_last:bool=False,\n                        run_box_and_prob_calculation:bool=True)\n\nThis is a wrapper for the YOLOX https://arxiv.org/abs/2107.08430 object detection model. The class handles preprocessing of the input, postprocessing of the model output, and calculation of bounding boxes and their probabilities.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nModule\n\nThe YOLOX model.\n\n\nnormalize_mean\nTensor\ntensor([[[[0.]], [[0.]], [[0.]]]])\nThe mean values for normalization.\n\n\nnormalize_std\nTensor\ntensor([[[[1.]], [[1.]], [[1.]]]])\nThe standard deviation values for normalization.\n\n\nstrides\nOptional\n[8, 16, 32]\nThe strides for the model.\n\n\nscale_inp\nbool\nFalse\nWhether to scale the input by dividing by 255.\n\n\nchannels_last\nbool\nFalse\nWhether the input tensor has channels first.\n\n\nrun_box_and_prob_calculation\nbool\nTrue\nWhether to calculate the bounding boxes and their probabilities.\n\n\n\n\nmodel_type = 'yolox_tiny'\n\nmodel = build_model(model_type, 19, pretrained=True)\n\ntest_inp = torch.randn(1, 3, 256, 256)\n\nwith torch.no_grad():\n    cls_scores, bbox_preds, objectness = model(test_inp)\n    \nprint(f\"cls_scores: {[cls_score.shape for cls_score in cls_scores]}\")\nprint(f\"bbox_preds: {[bbox_pred.shape for bbox_pred in bbox_preds]}\")\nprint(f\"objectness: {[objectness.shape for objectness in objectness]}\")\n\nThe file ./pretrained_checkpoints/yolox_tiny.pth already exists and overwrite is set to False.\ncls_scores: [torch.Size([1, 19, 32, 32]), torch.Size([1, 19, 16, 16]), torch.Size([1, 19, 8, 8])]\nbbox_preds: [torch.Size([1, 4, 32, 32]), torch.Size([1, 4, 16, 16]), torch.Size([1, 4, 8, 8])]\nobjectness: [torch.Size([1, 1, 32, 32]), torch.Size([1, 1, 16, 16]), torch.Size([1, 1, 8, 8])]\n\n\n/mnt/980_1TB_1/Projects/GitHub/cjm-yolox-pytorch/cjm_yolox_pytorch/model.py:792: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(checkpoint_path, map_location='cpu')\n\n\n\nnorm_stats = [*NORM_STATS[model_type].values()]\n\n# Convert the normalization stats to tensors\nmean_tensor = torch.tensor(norm_stats[0]).view(1, 3, 1, 1)\nstd_tensor = torch.tensor(norm_stats[1]).view(1, 3, 1, 1)\n\n# Set the model to evaluation mode\nmodel.eval();\n\n# Wrap the model with preprocessing and post-processing steps\nwrapped_model = YOLOXInferenceWrapper(model, \n                                      mean_tensor, \n                                      std_tensor, \n                                      scale_inp=False, \n                                      channels_last=False)\n\nwith torch.no_grad():\n    model_output = wrapped_model(test_inp)\nmodel_output.shape\n\ntorch.Size([1, 1344, 6])",
    "crumbs": [
      "inference"
    ]
  },
  {
    "objectID": "simota.html",
    "href": "simota.html",
    "title": "simota",
    "section": "",
    "text": "source\n\nAssignResult\n\n AssignResult (num_ground_truth_boxes:int,\n               ground_truth_box_indices:torch.LongTensor,\n               max_iou_values:torch.FloatTensor,\n               category_labels:torch.LongTensor=None)\n\n*Stores assignments between predicted bounding boxes and actual truth bounding boxes.\nBased on OpenMMLab’s implementation in the mmdetection library:\n\nOpenMMLab’s Implementation*\n\n\nsource\n\n\nSimOTAAssigner\n\n SimOTAAssigner (center_radius:float=2.5, candidate_topk:int=10,\n                 iou_weight:float=3.0, cls_weight:float=1.0)\n\n*The SimOTAAssigner class assigns predicted bounding boxes to their corresponding ground truth boxes in object detection tasks. It uses a process called SimOTA that formulates the assignment task as an optimal transport problem via a dynamic top-k strategy.\nIt calculates a cost matrix based on classification and regression (Intersection over Union, IoU) costs. It then uses this cost matrix to dynamically assign each ground truth object to the best matching bounding box predictions while resolving conflicts to ensure each prediction pairs with a single ground truth.\nBased on OpenMMLab’s implementation in the mmdetection library:\n\nOpenMMLab’s Implementation*\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncenter_radius\nfloat\n2.5\nGround truth center size to judge whether a output_grid_box is in center.\n\n\ncandidate_topk\nint\n10\nThe candidate top-k which used to get top-k ious to calculate dynamic-k.\n\n\niou_weight\nfloat\n3.0\nThe scale factor for regression iou cost.\n\n\ncls_weight\nfloat\n1.0\nThe scale factor for classification cost.\n\n\n\n\nsource\n\n\nSimOTAAssigner.assign\n\n SimOTAAssigner.assign (pred_scores:torch.Tensor,\n                        output_grid_boxes:torch.Tensor,\n                        decoded_bboxes:torch.Tensor,\n                        gt_bboxes:torch.Tensor, gt_labels:torch.Tensor,\n                        gt_bboxes_ignore:Optional[torch.Tensor]=None,\n                        eps:float=1e-07)\n\n*Assign ground truth to output_grid_boxes using SimOTA.\nThis method assigns predicted bounding boxes to ground truth boxes based on the computed cost matrix. It first extracts valid box predictions and scores. It then calculates the total cost matrix using IoU and classification costs. Finally, it uses the cost matrix to assign each prediction to a ground truth box.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npred_scores\nTensor\n\nClassification scores of each output grid box across all classes.\n\n\noutput_grid_boxes\nTensor\n\nOutput grid bounding boxes of one image in format [cx, xy, stride_w, stride_y].\n\n\ndecoded_bboxes\nTensor\n\nPredicted bounding boxes of one image in format [tl_x, tl_y, br_x, br_y].\n\n\ngt_bboxes\nTensor\n\nGround truth bounding boxes of one image in format [tl_x, tl_y, br_x, br_y].\n\n\ngt_labels\nTensor\n\nGround truth labels of one image, It is a Tensor with shape [num_gts].\n\n\ngt_bboxes_ignore\nOptional\nNone\nGround truth bounding boxes that are labelled as ignored, e.g., crowd boxes in COCO.\n\n\neps\nfloat\n1e-07\nA value added to the denominator for numerical stability.\n\n\n\n\nsource\n\n\nSimOTAAssigner.get_in_gt_and_in_center_info\n\n SimOTAAssigner.get_in_gt_and_in_center_info\n                                              (output_grid_boxes:torch.Ten\n                                              sor, gt_bboxes)\n\n*Get the information about whether output_grid_boxes are in ground truth boxes or center.\nThis method determines which predicted boxes are inside a ground truth box and also at the center of the ground truth box. It computes the centers of the ground truth boxes, checks if the predicted boxes are inside the ground truth boxes and centers, and then returns a mask indicating which predicted boxes are in either any ground truth box or any center box and which are in both.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\noutput_grid_boxes\nTensor\nAll output_grid_boxes of one image, a 2D-Tensor with shape [num_output_grid_boxes, 4] in [cx, xy, stride_w, stride_y] format.\n\n\ngt_bboxes\n\nGround truth bboxes of one image, a 2D-Tensor with shape [num_gts, 4] in [tl_x, tl_y, br_x, br_y] format.\n\n\nReturns\nTuple\nThe first tensor indicates if the output_grid_box is in any ground truth box or center, the second tensor specifies if the output_grid_box is in both the ground truth box and center.\n\n\n\n\nsource\n\n\nSimOTAAssigner.dynamic_k_matching\n\n SimOTAAssigner.dynamic_k_matching (cost:torch.Tensor,\n                                    pairwise_ious:torch.Tensor,\n                                    num_gt:int, valid_mask:torch.Tensor)\n\nThis method performs the dynamic k-matching process. For each ground truth box, it finds the top-k matching box predictions based on the smallest cost. If a predicted box matches multiple ground truths, it keeps only the one with the smallest cost. Finally, it returns the matched ground-truth indices and IoUs for valid predicted boxes.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncost\nTensor\nA 2D tensor representing the cost matrix calculated from both classification cost and regression IoU cost. Shape is [num_output_grid_boxes, num_gts].\n\n\npairwise_ious\nTensor\nA 2D tensor representing IoU scores between predictions and ground truths. Shape is [num_output_grid_boxes, num_gts].\n\n\nnum_gt\nint\nThe number of ground truth boxes.\n\n\nvalid_mask\nTensor\nA 1D tensor representing which predicted boxes are valid based on being in gt bboxes and in centers. Shape is [num_output_grid_boxes].\n\n\nReturns\nTuple\n(IoU scores for matched pairs, The indices of the ground truth for each output_grid_box)",
    "crumbs": [
      "simota"
    ]
  },
  {
    "objectID": "loss.html",
    "href": "loss.html",
    "title": "loss",
    "section": "",
    "text": "source\n\nSamplingResult\n\n SamplingResult (positive_indices:numpy.ndarray,\n                 negative_indices:numpy.ndarray, bboxes:numpy.ndarray,\n                 ground_truth_bboxes:torch.Tensor,\n                 assignment_result:cjm_yolox_pytorch.simota.AssignResult,\n                 ground_truth_flags:numpy.ndarray)\n\n*Bounding box sampling result.\nBased on OpenMMLab’s implementation in the mmdetection library:\n\nOpenMMLab’s Implementation*\n\n\nsource\n\n\nYOLOXLoss\n\n YOLOXLoss (num_classes:int, bbox_loss_weight:float=5.0,\n            class_loss_weight:float=1.0, objectness_loss_weight:float=1.0,\n            l1_loss_weight:float=1.0, use_l1:bool=False,\n            strides:List[int]=[8, 16, 32])\n\n*The callable YOLOXLoss class implements the loss function for training a YOLOX model.\nA YOLOXLoss instance takes the, class scores, predicted bounding boxes, objectness scores, ground truth bounding boxes, and ground truth labels. It then goes through the following steps:\n\nGenerate box coordinates for the output grids based on the input dimensions and stride values.\nFlatten and concatenate class predictions, bounding box predictions, and objectness scores.\nDecode box predictions.\nCompute targets for each image in the batch.\nConcatenate all positive masks, class targets, objectness targets, and bounding box targets.\nCompute the bounding box loss, objectness loss, and classification loss, scale them by their respective weights, and normalize them by the total number of samples.\nIf using L1 loss, concatenate L1 targets, computes the L1 loss, scale it by its weight, and normalize it by the total number of samples.\nReturn a dictionary containing the computed losses.\n\nBased on OpenMMLab’s implementation in the mmdetection library:\n\nOpenMMLab’s Implementation*\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_classes\nint\n\nThe number of target classes.\n\n\nbbox_loss_weight\nfloat\n5.0\nThe weight for the loss function to calculate the bounding box regression loss.\n\n\nclass_loss_weight\nfloat\n1.0\nThe weight for the loss function to calculate the classification loss.\n\n\nobjectness_loss_weight\nfloat\n1.0\nThe weight for the loss function to calculate the objectness loss.\n\n\nl1_loss_weight\nfloat\n1.0\nThe weight for the loss function to calculate the L1 loss.\n\n\nuse_l1\nbool\nFalse\nWhether to use L1 loss in the calculation.\n\n\nstrides\nList\n[8, 16, 32]\nThe list of strides.\n\n\n\n\nsource\n\n\nYOLOXLoss.__init__\n\n YOLOXLoss.__init__ (num_classes:int, bbox_loss_weight:float=5.0,\n                     class_loss_weight:float=1.0,\n                     objectness_loss_weight:float=1.0,\n                     l1_loss_weight:float=1.0, use_l1:bool=False,\n                     strides:List[int]=[8, 16, 32])\n\nThe __init__ method defines several parameters for computing the loss, and it initializes different loss functions, such as Generalized IoU for bounding box loss, binary cross entropy with logits for classification and objectness loss, and L1 loss if applicable.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_classes\nint\n\nThe number of target classes.\n\n\nbbox_loss_weight\nfloat\n5.0\nThe weight for the loss function to calculate the bounding box regression loss.\n\n\nclass_loss_weight\nfloat\n1.0\nThe weight for the loss function to calculate the classification loss.\n\n\nobjectness_loss_weight\nfloat\n1.0\nThe weight for the loss function to calculate the objectness loss.\n\n\nl1_loss_weight\nfloat\n1.0\nThe weight for the loss function to calculate the L1 loss.\n\n\nuse_l1\nbool\nFalse\nWhether to use L1 loss in the calculation.\n\n\nstrides\nList\n[8, 16, 32]\nThe list of strides.\n\n\n\n\nsource\n\n\nYOLOXLoss.bbox_decode\n\n YOLOXLoss.bbox_decode (output_grid_boxes:torch.Tensor,\n                        predicted_boxes:torch.Tensor)\n\nDecodes the predicted bounding boxes based on the output grid boxes. Positive indices are those where the ground truth box indices are greater-than zero (indicating a match with a ground truth object), and the negatives are where the ground truth box indices are zero (meaning it does not pair with a ground truth object).\n\n\n\n\nType\nDetails\n\n\n\n\noutput_grid_boxes\nTensor\nThe output grid boxes.\n\n\npredicted_boxes\nTensor\nThe predicted bounding boxes.\n\n\nReturns\nTensor\nThe decoded bounding boxes.\n\n\n\n\nsource\n\n\nYOLOXLoss.sample\n\n YOLOXLoss.sample\n                   (assignment_result:cjm_yolox_pytorch.simota.AssignResul\n                   t, bboxes:torch.Tensor,\n                   ground_truth_boxes:torch.Tensor)\n\nSamples positive and negative indices based on the assignment result.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nassignment_result\nAssignResult\nThe assignment result obtained from assigner.\n\n\nbboxes\nTensor\nThe predicted bounding boxes.\n\n\nground_truth_boxes\nTensor\nThe ground truth boxes.\n\n\nReturns\nSamplingResult\nThe sampling result containing positive and negative indices.\n\n\n\n\nsource\n\n\nYOLOXLoss.get_l1_target\n\n YOLOXLoss.get_l1_target (l1_target:torch.Tensor,\n                          ground_truth_boxes:torch.Tensor,\n                          output_grid_boxes:torch.Tensor,\n                          epsilon:float=1e-08)\n\nCalculates the L1 target, which measures the absolute differences between the predicted and actual values. The L1 loss measures how well the model’s predictions match the ground truth values.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nl1_target\nTensor\n\nThe L1 target tensor.\n\n\nground_truth_boxes\nTensor\n\nThe ground truth boxes.\n\n\noutput_grid_boxes\nTensor\n\nThe output grid boxes.\n\n\nepsilon\nfloat\n1e-08\nA small value to prevent division by zero.\n\n\nReturns\nTensor\n\nThe updated L1 target.\n\n\n\n\nsource\n\n\nYOLOXLoss.get_target_single\n\n YOLOXLoss.get_target_single (class_preds:torch.Tensor,\n                              objectness_score:torch.Tensor,\n                              output_grid_boxes:torch.Tensor,\n                              decoded_bboxes:torch.Tensor,\n                              ground_truth_bboxes:torch.Tensor,\n                              ground_truth_labels:torch.Tensor)\n\nCalculates the targets for a single image. It assigns ground truth objects to output grid boxes and samples output grid boxes based on the assignment results. It then generates class targets, objectness targets, bounding box targets, and, optionally, L1 targets.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nclass_preds\nTensor\nThe predicted class probabilities.\n\n\nobjectness_score\nTensor\nThe predicted objectness scores.\n\n\noutput_grid_boxes\nTensor\nThe output grid boxes.\n\n\ndecoded_bboxes\nTensor\nThe decoded bounding boxes.\n\n\nground_truth_bboxes\nTensor\nThe ground truth boxes.\n\n\nground_truth_labels\nTensor\nThe ground truth labels.\n\n\nReturns\nTuple\nThe targets for classification, objectness, bounding boxes, and L1 (if applicable), along with the foreground mask and the number of positive samples.\n\n\n\n\nsource\n\n\nYOLOXLoss.flatten_and_concat\n\n YOLOXLoss.flatten_and_concat (tensors:List[torch.Tensor], batch_size:int,\n                               reshape_dims:Optional[bool]=None)\n\nFlatten and concatenate a list of tensors.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntensors\nList\n\nA list of tensors to flatten and concatenate.\n\n\nbatch_size\nint\n\nThe batch size used to reshape the concatenated tensor.\n\n\nreshape_dims\nOptional\nNone\n\n\n\nReturns\nTensor\n\nThe concatenated tensor\n\n\n\n\nsource\n\n\nYOLOXLoss.__call__\n\n YOLOXLoss.__call__ (class_scores:List[torch.Tensor],\n                     predicted_bboxes:List[torch.Tensor],\n                     objectness_scores:List[torch.Tensor],\n                     ground_truth_bboxes:List[torch.Tensor],\n                     ground_truth_labels:List[torch.Tensor])\n\nThe __call__ method computes the loss values. It first generates box coordinates for the output grids based on the input dimensions and stride values. It then flattens and concatenates class predictions, bounding box predictions, and objectness scores. Next, it decodes the bounding box predictions, computes targets for each image in the batch, and finally computes the bounding box loss, objectness loss, and classification loss (and L1 loss, optionally). These losses are scaled by their respective weights and normalized by the total number of samples.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nclass_scores\nList\nA list of class scores for each scale.\n\n\npredicted_bboxes\nList\nA list of predicted bounding boxes for each scale.\n\n\nobjectness_scores\nList\nA list of objectness scores for each scale.\n\n\nground_truth_bboxes\nList\nA list of ground truth bounding boxes for each image.\n\n\nground_truth_labels\nList\nA list of ground truth labels for each image.\n\n\nReturns\nDict\nA dictionary with the classification, bounding box, objectness, and optionally, L1 loss.",
    "crumbs": [
      "loss"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\nmulti_apply\n\n multi_apply (func:Callable[...,Any], *args:Any, **kwargs:Any)\n\n*Applies the function func to each set of arguments in *args, possibly using keyword arguments **kwargs.\nBased on OpenMMLab’s implementation in the mmdetection library:\n\nOpenMMLab’s Implementation*\n\n\n\n\n\nType\nDetails\n\n\n\n\nfunc\nCallable\nFunction to apply.\n\n\nargs\nAny\n\n\n\nkwargs\nAny\n\n\n\nReturns\nTuple\n\n\n\n\n\nmulti_apply(lambda a, b: (a*2, b/2), [1, 2, 3, 4], [5, 6, 7, 8])\n\n([2, 4, 6, 8], [2.5, 3.0, 3.5, 4.0])\n\n\n\nsource\n\n\ngenerate_output_grids\n\n generate_output_grids (height, width, strides=[8, 16, 32])\n\n*Generate a tensor containing grid coordinates and strides for a given height and width.\nArgs: height (int): The height of the image. width (int): The width of the image.\nReturns: torch.Tensor: A tensor containing grid coordinates and strides.*\n\ngenerate_output_grids(32, 32)\n\ntensor([[ 0,  0,  8],\n        [ 1,  0,  8],\n        [ 2,  0,  8],\n        [ 3,  0,  8],\n        [ 0,  1,  8],\n        [ 1,  1,  8],\n        [ 2,  1,  8],\n        [ 3,  1,  8],\n        [ 0,  2,  8],\n        [ 1,  2,  8],\n        [ 2,  2,  8],\n        [ 3,  2,  8],\n        [ 0,  3,  8],\n        [ 1,  3,  8],\n        [ 2,  3,  8],\n        [ 3,  3,  8],\n        [ 0,  0, 16],\n        [ 1,  0, 16],\n        [ 0,  1, 16],\n        [ 1,  1, 16],\n        [ 0,  0, 32]])",
    "crumbs": [
      "utils"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cjm-yolox-pytorch",
    "section": "",
    "text": "pip install cjm_yolox_pytorch",
    "crumbs": [
      "cjm-yolox-pytorch"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "cjm-yolox-pytorch",
    "section": "",
    "text": "pip install cjm_yolox_pytorch",
    "crumbs": [
      "cjm-yolox-pytorch"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "cjm-yolox-pytorch",
    "section": "How to use",
    "text": "How to use\n\nimport torch\nfrom cjm_yolox_pytorch.model import MODEL_TYPES, build_model\n\nSelect model type\n\nmodel_type = MODEL_TYPES[0]\nmodel_type\n\n'yolox_tiny'\n\n\nBuild YOLOX model\n\nyolox = build_model(model_type, 19, pretrained=True)\n\ntest_inp = torch.randn(1, 3, 256, 256)\n\nwith torch.no_grad():\n    cls_scores, bbox_preds, objectness = yolox(test_inp)\n    \nprint(f\"cls_scores: {[cls_score.shape for cls_score in cls_scores]}\")\nprint(f\"bbox_preds: {[bbox_pred.shape for bbox_pred in bbox_preds]}\")\nprint(f\"objectness: {[objectness.shape for objectness in objectness]}\")\n\nThe file ./pretrained_checkpoints/yolox_tiny.pth already exists and overwrite is set to False.\ncls_scores: [torch.Size([1, 19, 32, 32]), torch.Size([1, 19, 16, 16]), torch.Size([1, 19, 8, 8])]\nbbox_preds: [torch.Size([1, 4, 32, 32]), torch.Size([1, 4, 16, 16]), torch.Size([1, 4, 8, 8])]\nobjectness: [torch.Size([1, 1, 32, 32]), torch.Size([1, 1, 16, 16]), torch.Size([1, 1, 8, 8])]",
    "crumbs": [
      "cjm-yolox-pytorch"
    ]
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "model",
    "section": "",
    "text": "model_type = MODEL_TYPES[0]\nmodel_type\n\n'yolox_tiny'\n\n\n\nsource\n\nConvModule\n\n ConvModule (in_channels:int, out_channels:int, kernel_size:int,\n             stride:int=1, padding:int=0, bias:bool=True, eps:float=1e-05,\n             momentum:float=0.1, affine:bool=True,\n             track_running_stats:bool=True, activation_function:Type[torch\n             .nn.modules.module.Module]=&lt;class\n             'torch.nn.modules.activation.SiLU'&gt;)\n\n*Configurable block used for Convolution2d-Normalization-Activation blocks.\n\nPseudocode\nFunction forward(input x):\n1. Pass the input (x) through the convolutional layer and store the result back to x.\n2. Pass the output from the convolutional layer (now stored in x) through the batch normalization layer and store the result back to x.\n3. Apply the activation function to the output of the batch normalization layer (x) and return the result.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n\nNumber of channels in the input image\n\n\nout_channels\nint\n\nNumber of channels produced by the convolution\n\n\nkernel_size\nint\n\nSize of the convolving kernel\n\n\nstride\nint\n1\nStride of the convolution.\n\n\npadding\nint\n0\nZero-padding added to both sides of the input.\n\n\nbias\nbool\nTrue\nIf set to False, the layer will not learn an additive bias.\n\n\neps\nfloat\n1e-05\nA value added to the denominator for numerical stability in BatchNorm2d.\n\n\nmomentum\nfloat\n0.1\nThe value used for the running_mean and running_var computation in BatchNorm2d.\n\n\naffine\nbool\nTrue\nIf set to True, this module has learnable affine parameters.\n\n\ntrack_running_stats\nbool\nTrue\nIf set to True, this module tracks the running mean and variance.\n\n\nactivation_function\nType\nSiLU\nThe activation function to be applied after batch normalization.\n\n\n\n\nsource\n\n\n\nDarknetBottleneck\n\n DarknetBottleneck (in_channels:int, out_channels:int, eps:float=0.001,\n                    momentum:float=0.03, affine:bool=True,\n                    track_running_stats:bool=True, add_identity:bool=True)\n\n*Basic Darknet bottleneck block used in Darknet.\nThis class represents a basic bottleneck block used in Darknet, which consists of two convolutional layers with a possible identity shortcut.\nBased on OpenMMLab’s implementation in the mmdetection library:\n\nOpenMMLab’s Implementation*\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n\nThe number of input channels to the block.\n\n\nout_channels\nint\n\nThe number of output channels from the block.\n\n\neps\nfloat\n0.001\nA value added to the denominator for numerical stability in the ConvModule’s BatchNorm layer.\n\n\nmomentum\nfloat\n0.03\nThe value used for the running_mean and running_var computation in the ConvModule’s BatchNorm layer.\n\n\naffine\nbool\nTrue\nA flag that when set to True, gives the ConvModule’s BatchNorm layer learnable affine parameters.\n\n\ntrack_running_stats\nbool\nTrue\nIf True, the ConvModule’s BatchNorm layer will track the running mean and variance.\n\n\nadd_identity\nbool\nTrue\nIf True, add an identity shortcut (also known as skip connection) to the output.\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nCSPLayer\n\n CSPLayer (in_channels:int, out_channels:int, num_blocks:int,\n           kernel_size:int=1, stride:int=1, padding:int=0,\n           eps:float=0.001, momentum:float=0.03, affine:bool=True,\n           track_running_stats:bool=True, add_identity:bool=True)\n\n*Cross Stage Partial Layer (CSPLayer).\nThis layer consists of a series of convolutions, blocks of transformations, and a final convolution. The inputs are processed via two paths: a main path with blocks and a shortcut path. The results from both paths are concatenated and further processed before returning the final output.\nThe blocks are instances of the DarknetBottleneck class which perform additional transformations.\nBased on OpenMMLab’s implementation in the mmdetection library:\n\nOpenMMLab’s Implementation*\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n\nNumber of input channels.\n\n\nout_channels\nint\n\nNumber of output channels.\n\n\nnum_blocks\nint\n\nNumber of blocks in the bottleneck.\n\n\nkernel_size\nint\n1\nSize of the convolving kernel.\n\n\nstride\nint\n1\nStride of the convolution.\n\n\npadding\nint\n0\nZero-padding added to both sides of the input.\n\n\neps\nfloat\n0.001\nA value added to the denominator for numerical stability.\n\n\nmomentum\nfloat\n0.03\nThe value used for the running_mean and running_var computation.\n\n\naffine\nbool\nTrue\nA flag that when set to True, gives the layer learnable affine parameters.\n\n\ntrack_running_stats\nbool\nTrue\nWhether or not to track the running mean and variance during training.\n\n\nadd_identity\nbool\nTrue\nWhether or not to add an identity shortcut connection if the input and output are the same size.\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nFocus\n\n Focus (in_channels:int, out_channels:int, kernel_size:int=1,\n        stride:int=1, bias:bool=False, eps:float=0.001,\n        momentum:float=0.03, affine:bool=True,\n        track_running_stats:bool=True)\n\n*Focus width and height information into channel space.\nBased on OpenMMLab’s implementation in the mmdetection library:\n\nOpenMMLab’s Implementation*\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n\nNumber of input channels.\n\n\nout_channels\nint\n\nNumber of output channels.\n\n\nkernel_size\nint\n1\nSize of the convolving kernel.\n\n\nstride\nint\n1\nStride of the convolution.\n\n\nbias\nbool\nFalse\nIf set to False, the layer will not learn an additive bias.\n\n\neps\nfloat\n0.001\nA value added to the denominator for numerical stability in the ConvModule’s BatchNorm layer.\n\n\nmomentum\nfloat\n0.03\nThe value used for the running_mean and running_var computation in the ConvModule’s BatchNorm layer.\n\n\naffine\nbool\nTrue\nA flag that when set to True, gives the ConvModule’s BatchNorm layer learnable affine parameters.\n\n\ntrack_running_stats\nbool\nTrue\nWhether or not to track the running mean and variance during training.\n\n\n\n\nsource\n\n\nSPPBottleneck\n\n SPPBottleneck (in_channels:int, out_channels:int,\n                pool_sizes:List[int]=[5, 9, 13], eps:float=0.001,\n                momentum:float=0.03, affine:bool=True,\n                track_running_stats:bool=True)\n\n*Spatial Pyramid Pooling layer used in YOLOv3-SPP\nBased on OpenMMLab’s implementation in the mmdetection library:\n\nOpenMMLab’s Implementation*\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n\nThe number of input channels.\n\n\nout_channels\nint\n\nThe number of output channels.\n\n\npool_sizes\nList\n[5, 9, 13]\nThe sizes of the pooling areas.\n\n\neps\nfloat\n0.001\nA value added to the denominator for numerical stability in the BatchNorm layer.\n\n\nmomentum\nfloat\n0.03\nThe value used for the running_mean and running_var computation in the BatchNorm layer.\n\n\naffine\nbool\nTrue\nA flag that when set to True, gives the BatchNorm layer learnable affine parameters.\n\n\ntrack_running_stats\nbool\nTrue\nWhether to keep track of running mean and variance in BatchNorm.\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nCSPDarknet\n\n CSPDarknet (arch='P5', deepen_factor=1.0, widen_factor=1.0,\n             out_indices=(2, 3, 4), spp_kernal_sizes=(5, 9, 13),\n             momentum=0.03, eps=0.001)\n\n*The CSPDarknet class implements a CSPDarknet backbone, a convolutional neural network (CNN) used in various image recognition tasks. The CSPDarknet backbone forms an integral part of the YOLOX object detection model.\nBased on OpenMMLab’s implementation in the mmdetection library:\n\nOpenMMLab’s Implementation*\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\narch\nstr\nP5\nArchitecture configuration, ‘P5’ or ‘P6’.\n\n\ndeepen_factor\nfloat\n1.0\nFactor to adjust the number of channels in each layer.\n\n\nwiden_factor\nfloat\n1.0\nFactor to adjust the number of blocks in CSP layer.\n\n\nout_indices\ntuple\n(2, 3, 4)\nIndices of the stages to output.\n\n\nspp_kernal_sizes\ntuple\n(5, 9, 13)\nSizes of the pooling operations in the Spatial Pyramid Pooling.\n\n\nmomentum\nfloat\n0.03\nMomentum for the moving average in batch normalization.\n\n\neps\nfloat\n0.001\nEpsilon for batch normalization to avoid numerical instability.\n\n\n\n\ncsp_darknet_cfg = CSP_DARKNET_CFGS[model_type]\ncsp_darknet = CSPDarknet(**csp_darknet_cfg)\n\nbackbone_inp = torch.randn(1, 3, 256, 256)\n\nwith torch.no_grad():\n    backbone_out = csp_darknet(backbone_inp)\n[out.shape for out in backbone_out]\n\n[torch.Size([1, 96, 32, 32]),\n torch.Size([1, 192, 16, 16]),\n torch.Size([1, 384, 8, 8])]\n\n\n\nsource\n\n\nYOLOXPAFPN\n\n YOLOXPAFPN (in_channels, out_channels, num_csp_blocks=3,\n             upsample_cfg={'scale_factor': 2, 'mode': 'nearest'},\n             momentum=0.03, eps=0.001)\n\n*Path Aggregation Feature Pyramid Network (PAFPN) used in YOLOX.\nIn object detection tasks, this class merges the feature maps from different layers of the backbone network. It helps in aggregating multi-scale feature maps to enhance the detection of objects of various sizes.\nBased on OpenMMLab’s implementation in the mmdetection library:\n\nOpenMMLab’s Implementation*\n\n\npafpn_cfg = PAFPN_CFGS[model_type]\nyolox_pafpn = YOLOXPAFPN(**pafpn_cfg)\n\nwith torch.no_grad():\n    neck_out = yolox_pafpn(backbone_out)\n[out.shape for out in neck_out]\n\n[torch.Size([1, 96, 32, 32]),\n torch.Size([1, 96, 16, 16]),\n torch.Size([1, 96, 8, 8])]\n\n\n\nsource\n\n\nYOLOXHead\n\n YOLOXHead (num_classes:int, in_channels:int, feat_channels=256,\n            stacked_convs=2, strides=[8, 16, 32], momentum=0.03,\n            eps=0.001)\n\n*The YOLOXHead class is a PyTorch module that implements the head of a YOLOX model https://arxiv.org/abs/2107.08430, used for bounding box prediction.\nThe head takes as input feature maps at multiple scale levels (e.g., from a feature pyramid network) and outputs predicted class scores, bounding box coordinates, and objectness scores for each scale level.\nBased on OpenMMLab’s implementation in the mmdetection library:\n\nOpenMMLab’s Implementation*\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_classes\nint\n\nThe number of target classes.\n\n\nin_channels\nint\n\nThe number of input channels.\n\n\nfeat_channels\nint\n256\nThe number of feature channels.\n\n\nstacked_convs\nint\n2\nThe number of convolution layers to stack.\n\n\nstrides\nlist\n[8, 16, 32]\nThe stride of each scale level in the feature pyramid.\n\n\nmomentum\nfloat\n0.03\nThe momentum for the moving average in batch normalization.\n\n\neps\nfloat\n0.001\nThe epsilon to avoid division by zero in batch normalization.\n\n\n\n\nhead_cfg = HEAD_CFGS[model_type]\nyolox_head = YOLOXHead(num_classes=80, **head_cfg)\n\nwith torch.no_grad():\n    cls_scores, bbox_preds, objectness = yolox_head(neck_out)    \nprint(f\"cls_scores: {[cls_score.shape for cls_score in cls_scores]}\")\nprint(f\"bbox_preds: {[bbox_pred.shape for bbox_pred in bbox_preds]}\")\nprint(f\"objectness: {[objectness.shape for objectness in objectness]}\")\n\ncls_scores: [torch.Size([1, 80, 32, 32]), torch.Size([1, 80, 16, 16]), torch.Size([1, 80, 8, 8])]\nbbox_preds: [torch.Size([1, 4, 32, 32]), torch.Size([1, 4, 16, 16]), torch.Size([1, 4, 8, 8])]\nobjectness: [torch.Size([1, 1, 32, 32]), torch.Size([1, 1, 16, 16]), torch.Size([1, 1, 8, 8])]\n\n\n\nsource\n\n\nYOLOX\n\n YOLOX (backbone:__main__.CSPDarknet, neck:__main__.YOLOXPAFPN,\n        bbox_head:__main__.YOLOXHead)\n\n*Implementation of YOLOX: Exceeding YOLO Series in 2021\n\nhttps://arxiv.org/abs/2107.08430\n\n\nPseudocode\nFunction forward(input_tensor x):\n\nPass x through the backbone module. The backbone module performs feature extraction from the input images. Store the output as ‘x’.\nPass the updated x through the neck module. The neck module performs feature aggregation of the extracted features. Update ‘x’ with the new output.\nPass the updated x through the bbox_head module. The bbox_head module predicts bounding boxes for potential objects in the images using the aggregated features. Update ‘x’ with the new output.\nReturn ‘x’ as the final output. The final ‘x’ represents the model’s predictions for object locations within the input images.*\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nbackbone\nCSPDarknet\nBackbone module for feature extraction.\n\n\nneck\nYOLOXPAFPN\nNeck module for feature aggregation.\n\n\nbbox_head\nYOLOXHead\nBbox head module for predicting bounding boxes.\n\n\n\n\nyolox = YOLOX(csp_darknet, yolox_pafpn, yolox_head)\n\nwith torch.no_grad():\n    cls_scores, bbox_preds, objectness = yolox(backbone_inp)    \nprint(f\"cls_scores: {[cls_score.shape for cls_score in cls_scores]}\")\nprint(f\"bbox_preds: {[bbox_pred.shape for bbox_pred in bbox_preds]}\")\nprint(f\"objectness: {[objectness.shape for objectness in objectness]}\")\n\ncls_scores: [torch.Size([1, 80, 32, 32]), torch.Size([1, 80, 16, 16]), torch.Size([1, 80, 8, 8])]\nbbox_preds: [torch.Size([1, 4, 32, 32]), torch.Size([1, 4, 16, 16]), torch.Size([1, 4, 8, 8])]\nobjectness: [torch.Size([1, 1, 32, 32]), torch.Size([1, 1, 16, 16]), torch.Size([1, 1, 8, 8])]\n\n\n\nsource\n\n\n\ninit_head\n\n init_head (head:__main__.YOLOXHead, num_classes:int)\n\n*Initialize the YOLOXHead with appropriate class outputs and convolution layers.\nThis function configures the output channels in the YOLOX head to match the number of classes in the dataset. It also initializes multiple level convolutional layers for each stride in the YOLOX head.*\n\n\n\n\nType\nDetails\n\n\n\n\nhead\nYOLOXHead\nThe YOLOX head to be initialized.\n\n\nnum_classes\nint\nThe number of classes in the dataset.\n\n\nReturns\nNone\n\n\n\n\n\nyolox_head.multi_level_conv_cls\n\nModuleList(\n  (0-2): 3 x Conv2d(96, 80, kernel_size=(1, 1), stride=(1, 1))\n)\n\n\n\ninit_head(yolox_head, 19)\nyolox_head.multi_level_conv_cls\n\nModuleList(\n  (0-2): 3 x Conv2d(96, 19, kernel_size=(1, 1), stride=(1, 1))\n)\n\n\n\nsource\n\n\nbuild_model\n\n build_model (model_type:str, num_classes:int, pretrained:bool=True,\n              checkpoint_dir:str='./pretrained_checkpoints/')\n\nBuilds a YOLOX model based on the given parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_type\nstr\n\nType of the model to be built.\n\n\nnum_classes\nint\n\nNumber of classes for the model.\n\n\npretrained\nbool\nTrue\nWhether to load pretrained weights.\n\n\ncheckpoint_dir\nstr\n./pretrained_checkpoints/\nDirectory to store checkpoints.\n\n\nReturns\nYOLOX\n\nThe built YOLOX model.\n\n\n\n\nyolox = build_model(model_type, 19, pretrained=True)\n\ntest_inp = torch.randn(1, 3, 256, 256)\n\nwith torch.no_grad():\n    cls_scores, bbox_preds, objectness = yolox(test_inp)\n    \nprint(f\"cls_scores: {[cls_score.shape for cls_score in cls_scores]}\")\nprint(f\"bbox_preds: {[bbox_pred.shape for bbox_pred in bbox_preds]}\")\nprint(f\"objectness: {[objectness.shape for objectness in objectness]}\")",
    "crumbs": [
      "model"
    ]
  }
]