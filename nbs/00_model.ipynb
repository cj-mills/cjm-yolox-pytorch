{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "\n",
    "> A PyTorch implementation of the [YOLOX](https://arxiv.org/abs/2107.08430) object detection model based on [OpenMMLab](https://github.com/open-mmlab)â€™s implementation in the [mmdetection](https://github.com/open-mmlab/mmdetection) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from typing import Any, Type, List, Optional, Callable, Tuple\n",
    "from functools import partial\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from cjm_yolox_pytorch.utils import multi_apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "MODEL_TYPES = ['yolox_tiny', 'yolox_s', 'yolox_m', 'yolox_l', 'yolox_x']\n",
    "\n",
    "CSP_DARKNET_CFGS = {\n",
    "    MODEL_TYPES[0]:dict(deepen_factor=0.33, widen_factor=0.375),\n",
    "    MODEL_TYPES[1]:dict(deepen_factor=0.33, widen_factor=0.5),\n",
    "    MODEL_TYPES[2]:dict(deepen_factor=0.67, widen_factor=0.75),\n",
    "    MODEL_TYPES[3]:dict(deepen_factor=1.0, widen_factor=1.0),\n",
    "    MODEL_TYPES[4]:dict(deepen_factor=1.33, widen_factor=1.25)\n",
    "}\n",
    "\n",
    "PAFPN_CFGS = {\n",
    "    MODEL_TYPES[0]:dict(in_channels=[96, 192, 384], out_channels=96, num_csp_blocks=1),\n",
    "    MODEL_TYPES[1]:dict(in_channels=[128, 256, 512], out_channels=128, num_csp_blocks=1),\n",
    "    MODEL_TYPES[2]:dict(in_channels=[192, 384, 768], out_channels=192, num_csp_blocks=2),\n",
    "    MODEL_TYPES[3]:dict(in_channels=[256, 512, 1024], out_channels=256, num_csp_blocks=3),\n",
    "    MODEL_TYPES[4]:dict(in_channels=[320, 640, 1280], out_channels=320, num_csp_blocks=4),\n",
    "}\n",
    "\n",
    "HEAD_CFGS = {\n",
    "    MODEL_TYPES[0]:dict(in_channels=96,feat_channels=96),\n",
    "    MODEL_TYPES[1]:dict(in_channels=128,feat_channels=128),\n",
    "    MODEL_TYPES[2]:dict(in_channels=192, feat_channels=192),\n",
    "    MODEL_TYPES[3]:dict(in_channels=256, feat_channels=256),\n",
    "    MODEL_TYPES[4]:dict(in_channels=320, feat_channels=320),\n",
    "}\n",
    "\n",
    "HUGGINGFACE_CKPT_URL = 'https://huggingface.co/cj-mills/yolox-coco-baseline-pytorch/resolve/main'\n",
    "\n",
    "PRETRAINED_URLS = {\n",
    "    MODEL_TYPES[0]:f'{HUGGINGFACE_CKPT_URL}/yolox_tiny.pth',\n",
    "    MODEL_TYPES[1]:f'{HUGGINGFACE_CKPT_URL}/yolox_s.pth',\n",
    "    MODEL_TYPES[2]:f'{HUGGINGFACE_CKPT_URL}/yolox_m.pth',\n",
    "    MODEL_TYPES[3]:f'{HUGGINGFACE_CKPT_URL}/yolox_l.pth',\n",
    "    MODEL_TYPES[4]:f'{HUGGINGFACE_CKPT_URL}/yolox_x.pth',\n",
    "}\n",
    "\n",
    "NORM_CFG = dict(momentum=0.03, eps=0.001)\n",
    "\n",
    "NORM_STATS = {\n",
    "    MODEL_TYPES[0]:dict(mean=(0.5, 0.5, 0.5), std=(1.0, 1.0, 1.0)),\n",
    "    MODEL_TYPES[1]:dict(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    MODEL_TYPES[2]:dict(mean=(0.5, 0.5, 0.5), std=(1.0, 1.0, 1.0)),\n",
    "    MODEL_TYPES[3]:dict(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    MODEL_TYPES[4]:dict(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "}\n",
    "\n",
    "MODEL_CFGS = {model_type: {**CSP_DARKNET_CFGS[model_type], \n",
    "                            **{'neck_'+k: v for k, v in PAFPN_CFGS[model_type].items()}, \n",
    "                            **{'head_'+k: v for k, v in HEAD_CFGS[model_type].items()}} \n",
    "               for model_type in MODEL_TYPES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yolox_tiny'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type = MODEL_TYPES[0]\n",
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConvModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Configurable block used for Convolution2d-Normalization-Activation blocks.\n",
    "    \n",
    "    #### Pseudocode\n",
    "    Function forward(input x):\n",
    "    \n",
    "        1. Pass the input (x) through the convolutional layer and store the result back to x.\n",
    "        2. Pass the output from the convolutional layer (now stored in x) through the batch normalization layer and store the result back to x.\n",
    "        3. Apply the activation function to the output of the batch normalization layer (x) and return the result.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_channels: int,  # Number of channels in the input image\n",
    "                 out_channels: int, # Number of channels produced by the convolution\n",
    "                 kernel_size: int,  # Size of the convolving kernel\n",
    "                 stride: int = 1,   # Stride of the convolution.\n",
    "                 padding: int = 0,  # Zero-padding added to both sides of the input.\n",
    "                 bias: bool = True, # If set to False, the layer will not learn an additive bias.\n",
    "                 eps: float = 1e-05,    # A value added to the denominator for numerical stability in BatchNorm2d.\n",
    "                 momentum: float = 0.1, # The value used for the running_mean and running_var computation in BatchNorm2d.\n",
    "                 affine: bool = True,   # If set to True, this module has learnable affine parameters.\n",
    "                 track_running_stats: bool = True, # If set to True, this module tracks the running mean and variance.\n",
    "                 activation_function: Type[nn.Module] = nn.SiLU # The activation function to be applied after batch normalization.\n",
    "                ):\n",
    "        \n",
    "        super(ConvModule, self).__init__()\n",
    "\n",
    "        # Convolutional layer\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
    "        # Batch normalization layer\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)\n",
    "        # Activation function\n",
    "        self.activate = activation_function()\n",
    "        \n",
    "        init.kaiming_normal_(self.conv.weight.data, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Pass input through convolutional layer\n",
    "        x = self.conv(x)\n",
    "        # Pass output from convolutional layer through batch normalization\n",
    "        x = self.bn(x)\n",
    "        # Apply activation function and return result\n",
    "        return self.activate(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DarknetBottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Darknet bottleneck block used in Darknet.\n",
    "    \n",
    "    This class represents a basic bottleneck block used in Darknet, which consists of two convolutional layers with a possible identity shortcut.\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "     - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/utils/csp_layer.py#L8)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels: int, # The number of input channels to the block.\n",
    "                 out_channels: int, # The number of output channels from the block.\n",
    "                 eps: float = 0.001, # A value added to the denominator for numerical stability in the ConvModule's BatchNorm layer.\n",
    "                 momentum: float = 0.03, # The value used for the running_mean and running_var computation in the ConvModule's BatchNorm layer.\n",
    "                 affine: bool = True, # A flag that when set to True, gives the ConvModule's BatchNorm layer learnable affine parameters.\n",
    "                 track_running_stats: bool = True, # If True, the ConvModule's BatchNorm layer will track the running mean and variance.\n",
    "                 add_identity: bool = True # If True, add an identity shortcut (also known as skip connection) to the output.\n",
    "                ) -> None:\n",
    "        super(DarknetBottleneck, self).__init__()\n",
    "\n",
    "        self.add_identity = add_identity\n",
    "\n",
    "        # The first conv layer reduces the dimensionality with a 1x1 kernel, \n",
    "        # and the second conv layer restores it with a 3x3 kernel.\n",
    "        self.conv1 = ConvModule(in_channels, out_channels, kernel_size=1, stride=1, padding=0, \n",
    "                                bias=False, eps=eps, momentum=momentum, affine=affine, \n",
    "                                track_running_stats=track_running_stats)\n",
    "        self.conv2 = ConvModule(out_channels, out_channels, kernel_size=3, stride=1, padding=1, \n",
    "                                bias=False, eps=eps, momentum=momentum, affine=affine, \n",
    "                                track_running_stats=track_running_stats)\n",
    "        \n",
    "        # If add_identity is True and in_channels do not match out_channels, \n",
    "        # introduce a conv layer on the identity shortcut to match the dimensions.\n",
    "        # If not, use nn.Identity() for a cleaner forward method.\n",
    "        if self.add_identity and in_channels != out_channels:\n",
    "            self.identity_conv = ConvModule(in_channels, out_channels, kernel_size=1, stride=1, padding=0, \n",
    "                                            bias=False, eps=eps, momentum=momentum, affine=affine, \n",
    "                                            track_running_stats=track_running_stats)\n",
    "        else:\n",
    "            self.identity_conv = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "        out = self.conv2(self.conv1(x))\n",
    "        \n",
    "        # If add_identity is True, add the transformed (if necessary) identity to the output\n",
    "        if self.add_identity:\n",
    "            out += self.identity_conv(identity)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CSPLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross Stage Partial Layer (CSPLayer).\n",
    "    \n",
    "    This layer consists of a series of convolutions, blocks of transformations, and a final convolution. \n",
    "    The inputs are processed via two paths: a main path with blocks and a shortcut path. The results from \n",
    "    both paths are concatenated and further processed before returning the final output.\n",
    "\n",
    "    The blocks are instances of the DarknetBottleneck class which perform additional transformations.\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/)\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels: int, # Number of input channels.\n",
    "                 out_channels: int, # Number of output channels.\n",
    "                 num_blocks: int, # Number of blocks in the bottleneck.\n",
    "                 kernel_size: int = 1, # Size of the convolving kernel.\n",
    "                 stride: int = 1, # Stride of the convolution.\n",
    "                 padding: int = 0, # Zero-padding added to both sides of the input.\n",
    "                 eps: float = 0.001, # A value added to the denominator for numerical stability.\n",
    "                 momentum: float = 0.03, # The value used for the running_mean and running_var computation.\n",
    "                 affine: bool = True, # A flag that when set to True, gives the layer learnable affine parameters.\n",
    "                 track_running_stats: bool = True, # Whether or not to track the running mean and variance during training.\n",
    "                 add_identity: bool = True # Whether or not to add an identity shortcut connection if the input and output are the same size.\n",
    "                ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        hidden_channels = out_channels // 2\n",
    "\n",
    "        conv_params = {\n",
    "            'kernel_size': kernel_size, \n",
    "            'stride': stride, \n",
    "            'padding': padding, \n",
    "            'bias': False, \n",
    "            'eps': eps, \n",
    "            'momentum': momentum, \n",
    "            'affine': affine, \n",
    "            'track_running_stats': track_running_stats\n",
    "        }\n",
    "\n",
    "        self.main_conv = ConvModule(in_channels=in_channels, out_channels=hidden_channels, **conv_params)\n",
    "        self.short_conv = ConvModule(in_channels=in_channels, out_channels=hidden_channels, **conv_params)\n",
    "\n",
    "        self.final_conv = ConvModule(in_channels=2 * hidden_channels, out_channels=out_channels, **conv_params)\n",
    "\n",
    "        block_params = {\n",
    "            'in_channels': hidden_channels, \n",
    "            'out_channels': hidden_channels, \n",
    "            'eps': eps, \n",
    "            'momentum': momentum, \n",
    "            'affine': affine, \n",
    "            'track_running_stats': track_running_stats, \n",
    "            'add_identity': add_identity\n",
    "        }\n",
    "\n",
    "        self.blocks = nn.ModuleList([DarknetBottleneck(**block_params) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        main_path = self.main_conv(x)\n",
    "        for block in self.blocks:\n",
    "            main_path = block(main_path)\n",
    "\n",
    "        shortcut_path = self.short_conv(x)\n",
    "\n",
    "        return self.final_conv(torch.cat((main_path, shortcut_path), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Focus(nn.Module):\n",
    "    \"\"\"\n",
    "    Focus width and height information into channel space.\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/backbones/csp_darknet.py#L14)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels: int, # Number of input channels.\n",
    "                 out_channels: int, # Number of output channels.\n",
    "                 kernel_size: int = 1, # Size of the convolving kernel.\n",
    "                 stride: int = 1, # Stride of the convolution.\n",
    "                 bias: bool = False, # If set to False, the layer will not learn an additive bias.\n",
    "                 eps: float = 0.001, #  A value added to the denominator for numerical stability in the ConvModule's BatchNorm layer.\n",
    "                 momentum: float = 0.03, # The value used for the running_mean and running_var computation in the ConvModule's BatchNorm layer.\n",
    "                 affine: bool = True, # A flag that when set to True, gives the ConvModule's BatchNorm layer learnable affine parameters.\n",
    "                 track_running_stats: bool = True # Whether or not to track the running mean and variance during training.\n",
    "                ):\n",
    "        \n",
    "        super(Focus, self).__init__()\n",
    "        self.conv = ConvModule(\n",
    "            in_channels * 4,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding=(kernel_size - 1) // 2,\n",
    "            bias=bias,\n",
    "            eps=eps,\n",
    "            momentum=momentum,\n",
    "            affine=affine,\n",
    "            track_running_stats=track_running_stats)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "                \n",
    "        # Split the input tensor into 4 patches\n",
    "        patch_top_left = x[..., ::2, ::2]   # Top left patch\n",
    "        patch_top_right = x[..., ::2, 1::2]  # Top right patch\n",
    "        patch_bot_left = x[..., 1::2, ::2]  # Bottom left patch\n",
    "        patch_bot_right = x[..., 1::2, 1::2]  # Bottom right patch\n",
    "        \n",
    "        # Concatenate the patches along the channel dimension in order respecting the spatial locality\n",
    "        x = torch.cat(\n",
    "            (\n",
    "                patch_top_left,\n",
    "                patch_top_right,\n",
    "                patch_bot_left,\n",
    "                patch_bot_right,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SPPBottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatial Pyramid Pooling layer used in YOLOv3-SPP\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/backbones/csp_darknet.py#L67)\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels: int, # The number of input channels.\n",
    "                 out_channels: int, # The number of output channels.\n",
    "                 pool_sizes: List[int] = [5, 9, 13], # The sizes of the pooling areas.\n",
    "                 eps: float = 0.001, # A value added to the denominator for numerical stability in the BatchNorm layer.\n",
    "                 momentum: float = 0.03, #  The value used for the running_mean and running_var computation in the BatchNorm layer.\n",
    "                 affine: bool = True, # A flag that when set to True, gives the BatchNorm layer learnable affine parameters.\n",
    "                 track_running_stats: bool = True # Whether to keep track of running mean and variance in BatchNorm.\n",
    "                ) -> None:\n",
    "        \n",
    "        super(SPPBottleneck, self).__init__()\n",
    "\n",
    "        hidden_channels = in_channels // 2\n",
    "\n",
    "        self.conv1 = ConvModule(in_channels, hidden_channels, kernel_size=1, stride=1, padding=0, \n",
    "                                bias=False, eps=eps, momentum=momentum, affine=affine, \n",
    "                                track_running_stats=track_running_stats)\n",
    "\n",
    "        self.pooling_layers = nn.ModuleList([nn.MaxPool2d(kernel_size=ps, stride=1, padding=ps//2) for ps in pool_sizes])\n",
    "\n",
    "        self.conv2 = ConvModule(hidden_channels * (len(pool_sizes) + 1), out_channels, kernel_size=1, stride=1, padding=0, \n",
    "                                bias=False, eps=eps, momentum=momentum, affine=affine, \n",
    "                                track_running_stats=track_running_stats)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "                \n",
    "        x = self.conv1(x)\n",
    "\n",
    "        pooling_results = [x]\n",
    "        for pooling in self.pooling_layers:\n",
    "            pooling_results.append(pooling(x))\n",
    "\n",
    "        x = torch.cat(pooling_results, dim=1)\n",
    "\n",
    "        return self.conv2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CSPDarknet(nn.Module):\n",
    "    \"\"\"\n",
    "    The `CSPDarknet` class implements a CSPDarknet backbone, a convolutional neural network (CNN) used in various image recognition tasks. The CSPDarknet backbone forms an integral part of the YOLOX object detection model.\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/backbones/csp_darknet.py#L124)\n",
    "    \"\"\"\n",
    "\n",
    "    # Architecture settings for P5 and P6\n",
    "    ARCH_SETTINGS = {\n",
    "        'P5': [[64, 128, 3, True, False], [128, 256, 9, True, False],\n",
    "               [256, 512, 9, True, False], [512, 1024, 3, False, True]],\n",
    "        'P6': [[64, 128, 3, True, False], [128, 256, 9, True, False],\n",
    "               [256, 512, 9, True, False], [512, 768, 3, True, False],\n",
    "               [768, 1024, 3, False, True]]\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 arch='P5', # Architecture configuration, 'P5' or 'P6'.\n",
    "                 deepen_factor=1.0, # Factor to adjust the number of channels in each layer.\n",
    "                 widen_factor=1.0, # Factor to adjust the number of blocks in CSP layer.\n",
    "                 out_indices=(2, 3, 4), # Indices of the stages to output.\n",
    "                 spp_kernal_sizes=(5, 9, 13), # Sizes of the pooling operations in the Spatial Pyramid Pooling.\n",
    "                 momentum=0.03, # Momentum for the moving average in batch normalization.\n",
    "                 eps=0.001 # Epsilon for batch normalization to avoid numerical instability.\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        if not set(out_indices).issubset(range(len(self.ARCH_SETTINGS[arch]) + 1)):\n",
    "            raise ValueError(\"out_indices are out of range\")\n",
    "\n",
    "        self.out_indices = out_indices\n",
    "        # Building the initial layer of the model\n",
    "        self.stem = Focus(\n",
    "            3,\n",
    "            int(self.ARCH_SETTINGS[arch][0][0] * widen_factor),\n",
    "            kernel_size=3,\n",
    "            stride=1\n",
    "            )\n",
    "        self.layers = ['stem']\n",
    "        # Building the stages of the model\n",
    "        self._build_stages(arch, deepen_factor, widen_factor, spp_kernal_sizes, momentum, eps)\n",
    "\n",
    "    def _build_stages(self, arch, deepen_factor, widen_factor, spp_kernal_sizes, momentum, eps):\n",
    "        \"\"\"\n",
    "        Build the stages of the CSPDarknet model.\n",
    "\n",
    "        Args:\n",
    "            arch (str): Architecture type, 'P5' or 'P6'.\n",
    "            deepen_factor (float): Factor to adjust the depth of the model.\n",
    "            widen_factor (float): Factor to adjust the width of the model.\n",
    "            spp_kernal_sizes (tuple): Sizes of the pooling operations in the Spatial Pyramid Pooling.\n",
    "            momentum (float): Momentum for the moving average in batch normalization.\n",
    "            eps (float): Epsilon for batch normalization to avoid numerical instability.\n",
    "        \"\"\"\n",
    "\n",
    "        # For each stage configuration in the architecture settings\n",
    "        for i, (in_c, out_c, num_blocks, add_identity, use_spp) in enumerate(self.ARCH_SETTINGS[arch]):\n",
    "            # Adjust the channel size based on the widen factor\n",
    "            in_c, out_c = int(in_c * widen_factor), int(out_c * widen_factor)\n",
    "            # Adjust the number of blocks based on the deepen factor\n",
    "            num_blocks = max(round(num_blocks * deepen_factor), 1)\n",
    "\n",
    "            stage = []\n",
    "\n",
    "            # Append ConvModule for the stage\n",
    "            stage.append(ConvModule(in_c, \n",
    "                                    out_c, 3, \n",
    "                                    stride=2, \n",
    "                                    padding=1, \n",
    "                                    bias=False, \n",
    "                                    eps=eps, \n",
    "                                    momentum=momentum, \n",
    "                                    affine=True, \n",
    "                                    track_running_stats=True))\n",
    "\n",
    "            # If use_spp is True, append a Spatial Pyramid Pooling layer\n",
    "            if use_spp:\n",
    "                stage.append(SPPBottleneck(out_c, out_c, pool_sizes=spp_kernal_sizes))\n",
    "\n",
    "            # Append a Cross Stage Partial layer\n",
    "            stage.append(CSPLayer(out_c, out_c, num_blocks=num_blocks, add_identity=add_identity))\n",
    "            # Add the stage to the model as a sequential layer\n",
    "            self.add_module(f'stage{i + 1}', nn.Sequential(*stage))\n",
    "            self.layers.append(f'stage{i + 1}')\n",
    "\n",
    "    def forward(self, x):\n",
    "                \n",
    "        outs = []\n",
    "        # For each layer in the model\n",
    "        for i, layer_name in enumerate(self.layers):\n",
    "            # Get the layer by its name\n",
    "            layer = getattr(self, layer_name)\n",
    "            # Pass the input through the layer\n",
    "            x = layer(x)\n",
    "            # If the index is in out_indices, append the output to outs\n",
    "            if i in self.out_indices:\n",
    "                outs.append(x)\n",
    "        return tuple(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 96, 32, 32]),\n",
       " torch.Size([1, 192, 16, 16]),\n",
       " torch.Size([1, 384, 8, 8])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csp_darknet_cfg = CSP_DARKNET_CFGS[model_type]\n",
    "csp_darknet = CSPDarknet(**csp_darknet_cfg)\n",
    "\n",
    "backbone_inp = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    backbone_out = csp_darknet(backbone_inp)\n",
    "[out.shape for out in backbone_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class YOLOXPAFPN(nn.Module):\n",
    "    \"\"\"\n",
    "    Path Aggregation Feature Pyramid Network (PAFPN) used in YOLOX.\n",
    "    \n",
    "    In object detection tasks, this class merges the feature maps from different layers of the backbone network. It helps in aggregating multi-scale feature maps to enhance the detection of objects of various sizes.\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/necks/yolox_pafpn.py#L14)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 num_csp_blocks=3,\n",
    "                 upsample_cfg=dict(scale_factor=2, mode='nearest'),\n",
    "                 momentum=0.03,\n",
    "                 eps=0.001):\n",
    "        super(YOLOXPAFPN, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.upsample = nn.Upsample(**upsample_cfg)\n",
    "\n",
    "        # build top-down blocks, which includes reduce layers and CSP blocks\n",
    "        self.reduce_layers = nn.ModuleList([\n",
    "            ConvModule(\n",
    "                in_channels[idx],\n",
    "                in_channels[idx - 1],\n",
    "                kernel_size=(1, 1),\n",
    "                stride=(1, 1),\n",
    "                padding=0,\n",
    "                bias=False,\n",
    "                momentum=momentum,\n",
    "                eps=eps,\n",
    "                affine=True,\n",
    "                track_running_stats=True\n",
    "            ) for idx in range(len(in_channels) - 1, 0, -1)\n",
    "        ])\n",
    "        self.top_down_blocks = nn.ModuleList([\n",
    "            CSPLayer(\n",
    "                in_channels[idx - 1] * 2,\n",
    "                in_channels[idx - 1],\n",
    "                num_blocks=num_csp_blocks,\n",
    "                add_identity=False\n",
    "            ) for idx in range(len(in_channels) - 1, 0, -1)\n",
    "        ])\n",
    "\n",
    "        # build bottom-up blocks, which includes downsampling layers and CSP blocks\n",
    "        self.downsamples = nn.ModuleList([\n",
    "            ConvModule(\n",
    "                in_channels[idx],\n",
    "                in_channels[idx],\n",
    "                3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "                momentum=momentum,\n",
    "                eps=eps,\n",
    "                affine=True,\n",
    "                track_running_stats=True\n",
    "            ) for idx in range(len(in_channels) - 1)\n",
    "        ])\n",
    "        self.bottom_up_blocks = nn.ModuleList([\n",
    "            CSPLayer(\n",
    "                in_channels[idx] * 2,\n",
    "                in_channels[idx + 1],\n",
    "                num_blocks=num_csp_blocks,\n",
    "                add_identity=False\n",
    "            ) for idx in range(len(in_channels) - 1)\n",
    "        ])\n",
    "\n",
    "        # build output convolutions for each level\n",
    "        self.out_convs = nn.ModuleList([\n",
    "            ConvModule(\n",
    "                in_channels[i],\n",
    "                out_channels,\n",
    "                1,\n",
    "                stride=(1, 1),\n",
    "                padding=0,\n",
    "                bias=False,\n",
    "                momentum=momentum,\n",
    "                eps=eps,\n",
    "                affine=True,\n",
    "                track_running_stats=True\n",
    "            ) for i in range(len(in_channels))\n",
    "        ])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        assert len(inputs) == len(self.in_channels)\n",
    "\n",
    "        # top-down path\n",
    "        inner_outs = self._top_down(inputs)\n",
    "\n",
    "        # bottom-up path\n",
    "        outs = self._bottom_up(inner_outs)\n",
    "\n",
    "        # apply the output convolutions to the feature maps\n",
    "        outs = [conv(out) for out, conv in zip(outs, self.out_convs)]\n",
    "\n",
    "        return tuple(outs)\n",
    "\n",
    "    def _top_down(self, inputs):\n",
    "        inner_outs = [inputs[-1]]\n",
    "        for idx, (reduce_layer, block) in enumerate(zip(self.reduce_layers, self.top_down_blocks)):\n",
    "            feat_high = reduce_layer(inner_outs[0])\n",
    "            inner_outs[0] = feat_high\n",
    "            upsample_feat = self.upsample(feat_high)\n",
    "            inner_out = block(torch.cat([upsample_feat, inputs[len(inputs) - 2 - idx]], 1))\n",
    "            inner_outs.insert(0, inner_out)\n",
    "        return inner_outs\n",
    "\n",
    "    def _bottom_up(self, inner_outs):\n",
    "        outs = [inner_outs[0]]\n",
    "        for idx, (downsample, block) in enumerate(zip(self.downsamples, self.bottom_up_blocks)):\n",
    "            downsample_feat = downsample(outs[-1])\n",
    "            out = block(torch.cat([downsample_feat, inner_outs[idx + 1]], 1))\n",
    "            outs.append(out)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 96, 32, 32]),\n",
       " torch.Size([1, 96, 16, 16]),\n",
       " torch.Size([1, 96, 8, 8])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pafpn_cfg = PAFPN_CFGS[model_type]\n",
    "yolox_pafpn = YOLOXPAFPN(**pafpn_cfg)\n",
    "\n",
    "with torch.no_grad():\n",
    "    neck_out = yolox_pafpn(backbone_out)\n",
    "[out.shape for out in neck_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class YOLOXHead(nn.Module):\n",
    "    \"\"\"\n",
    "    The `YOLOXHead` class is a PyTorch module that implements the head of a YOLOX model <https://arxiv.org/abs/2107.08430>, used for bounding box prediction.\n",
    "    \n",
    "    The head takes as input feature maps at multiple scale levels (e.g., from a feature pyramid network) and outputs predicted class scores, bounding box coordinates, and objectness scores for each scale level.\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/dense_heads/yolox_head.py#L20)\n",
    "    \"\"\"\n",
    "    \n",
    "    BBOX_DIM = 4\n",
    "    OBJECTNESS_DIM = 1\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_classes:int, # The number of target classes.\n",
    "                 in_channels:int, # The number of input channels.\n",
    "                 feat_channels=256, # The number of feature channels.\n",
    "                 stacked_convs=2, # The number of convolution layers to stack.\n",
    "                 strides=[8, 16, 32], # The stride of each scale level in the feature pyramid.\n",
    "                 momentum=0.03, # The momentum for the moving average in batch normalization.\n",
    "                 eps=0.001 # The epsilon to avoid division by zero in batch normalization.\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "        # Store various configurations as instance variables\n",
    "        self.num_classes = num_classes\n",
    "        self.cls_out_channels = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.feat_channels = feat_channels\n",
    "        self.stacked_convs = stacked_convs\n",
    "        self.strides = strides\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        \n",
    "        # Initialize the layers of the model\n",
    "        self._init_layers()\n",
    "\n",
    "    def _init_layers(self):\n",
    "        \"\"\"\n",
    "        Initialize layers for the head module, includes classification \n",
    "        convolutions, regression convolutions and prediction convolutions \n",
    "        for each scale level.\n",
    "        \"\"\"\n",
    "        # Initialize multi-level lists for each type of layer\n",
    "        self.multi_level_cls_convs = nn.ModuleList()\n",
    "        self.multi_level_reg_convs = nn.ModuleList()\n",
    "        self.multi_level_conv_cls = nn.ModuleList()\n",
    "        self.multi_level_conv_reg = nn.ModuleList()\n",
    "        self.multi_level_conv_obj = nn.ModuleList()\n",
    "        \n",
    "        # For each stride level, create layers and add them to their respective lists\n",
    "        for _ in self.strides:\n",
    "            self.multi_level_cls_convs.append(self._build_stacked_convs())\n",
    "            self.multi_level_reg_convs.append(self._build_stacked_convs())\n",
    "            conv_cls, conv_reg, conv_obj = self._build_predictor()\n",
    "            self.multi_level_conv_cls.append(conv_cls)\n",
    "            self.multi_level_conv_reg.append(conv_reg)\n",
    "            self.multi_level_conv_obj.append(conv_obj)\n",
    "\n",
    "    def _build_stacked_convs(self):\n",
    "        \"\"\"\n",
    "        Build stacked convolution layers.\n",
    "        \"\"\"\n",
    "        conv = ConvModule\n",
    "        stacked_convs = []\n",
    "        # Create a series of convolution layers\n",
    "        for i in range(self.stacked_convs):\n",
    "            chn = self.in_channels if i == 0 else self.feat_channels\n",
    "            stacked_convs.append(\n",
    "                conv(\n",
    "                    chn,\n",
    "                    self.feat_channels,\n",
    "                    3,\n",
    "                    stride=1,\n",
    "                    padding=1,\n",
    "                    momentum=self.momentum,\n",
    "                    eps=self.eps,\n",
    "                    affine=True, \n",
    "                    track_running_stats=True,\n",
    "                    bias=False))\n",
    "        # Return the layers as a sequential model\n",
    "        return nn.Sequential(*stacked_convs)\n",
    "\n",
    "    def _build_predictor(self):\n",
    "        \"\"\"\n",
    "        Build predictor layers for classification, regression, and objectness.\n",
    "        \"\"\"\n",
    "        # Create convolution layers for each type of prediction\n",
    "        conv_cls = nn.Conv2d(self.feat_channels, self.cls_out_channels, 1)\n",
    "        conv_reg = nn.Conv2d(self.feat_channels, self.BBOX_DIM, 1)\n",
    "        conv_obj = nn.Conv2d(self.feat_channels, self.OBJECTNESS_DIM, 1)\n",
    "        return conv_cls, conv_reg, conv_obj\n",
    "\n",
    "    def forward_single(self, x, cls_convs, reg_convs, conv_cls, conv_reg, conv_obj):\n",
    "        \"\"\"\n",
    "        Forward feature of a single scale level.\n",
    "        \"\"\"\n",
    "        # Pass input through classification and regression convolutions\n",
    "        cls_feat = cls_convs(x)\n",
    "        reg_feat = reg_convs(x)\n",
    "\n",
    "        # Apply predictors to get scores and predictions\n",
    "        cls_score = conv_cls(cls_feat)\n",
    "        bbox_pred = conv_reg(reg_feat)\n",
    "        objectness = conv_obj(reg_feat)\n",
    "\n",
    "        return cls_score, bbox_pred, objectness\n",
    "\n",
    "    def forward(self, feats):\n",
    "        \"\"\"\n",
    "        Forward pass for the head.\n",
    "        \"\"\"\n",
    "        # Apply the forward_single function to each scale level\n",
    "        return multi_apply(self.forward_single, feats,\n",
    "                           self.multi_level_cls_convs,\n",
    "                           self.multi_level_reg_convs,\n",
    "                           self.multi_level_conv_cls,\n",
    "                           self.multi_level_conv_reg,\n",
    "                           self.multi_level_conv_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_scores: [torch.Size([1, 80, 32, 32]), torch.Size([1, 80, 16, 16]), torch.Size([1, 80, 8, 8])]\n",
      "bbox_preds: [torch.Size([1, 4, 32, 32]), torch.Size([1, 4, 16, 16]), torch.Size([1, 4, 8, 8])]\n",
      "objectness: [torch.Size([1, 1, 32, 32]), torch.Size([1, 1, 16, 16]), torch.Size([1, 1, 8, 8])]\n"
     ]
    }
   ],
   "source": [
    "head_cfg = HEAD_CFGS[model_type]\n",
    "yolox_head = YOLOXHead(num_classes=80, **head_cfg)\n",
    "\n",
    "with torch.no_grad():\n",
    "    cls_scores, bbox_preds, objectness = yolox_head(neck_out)    \n",
    "print(f\"cls_scores: {[cls_score.shape for cls_score in cls_scores]}\")\n",
    "print(f\"bbox_preds: {[bbox_pred.shape for bbox_pred in bbox_preds]}\")\n",
    "print(f\"objectness: {[objectness.shape for objectness in objectness]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class YOLOX(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of `YOLOX: Exceeding YOLO Series in 2021`\n",
    "    \n",
    "    - <https://arxiv.org/abs/2107.08430>\n",
    "    \n",
    "    #### Pseudocode\n",
    "    Function forward(input_tensor x):\n",
    "\n",
    "    1. Pass x through the backbone module. The backbone module performs feature extraction from the input images. Store the output as 'x'.\n",
    "    2. Pass the updated x through the neck module. The neck module performs feature aggregation of the extracted features. Update 'x' with the new output.\n",
    "    3. Pass the updated x through the bbox_head module. The bbox_head module predicts bounding boxes for potential objects in the images using the aggregated features. Update 'x' with the new output.\n",
    "    4. Return 'x' as the final output. The final 'x' represents the model's predictions for object locations within the input images.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 backbone:CSPDarknet, # Backbone module for feature extraction.\n",
    "                 neck:YOLOXPAFPN, # Neck module for feature aggregation.\n",
    "                 bbox_head:YOLOXHead): # Bbox head module for predicting bounding boxes.\n",
    "        super(YOLOX, self).__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.neck = neck\n",
    "        self.bbox_head = bbox_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward through backbone\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        # Forward through neck\n",
    "        x = self.neck(x)\n",
    "\n",
    "        # Forward through bbox_head\n",
    "        x = self.bbox_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_scores: [torch.Size([1, 80, 32, 32]), torch.Size([1, 80, 16, 16]), torch.Size([1, 80, 8, 8])]\n",
      "bbox_preds: [torch.Size([1, 4, 32, 32]), torch.Size([1, 4, 16, 16]), torch.Size([1, 4, 8, 8])]\n",
      "objectness: [torch.Size([1, 1, 32, 32]), torch.Size([1, 1, 16, 16]), torch.Size([1, 1, 8, 8])]\n"
     ]
    }
   ],
   "source": [
    "yolox = YOLOX(csp_darknet, yolox_pafpn, yolox_head)\n",
    "\n",
    "with torch.no_grad():\n",
    "    cls_scores, bbox_preds, objectness = yolox(backbone_inp)    \n",
    "print(f\"cls_scores: {[cls_score.shape for cls_score in cls_scores]}\")\n",
    "print(f\"bbox_preds: {[bbox_pred.shape for bbox_pred in bbox_preds]}\")\n",
    "print(f\"objectness: {[objectness.shape for objectness in objectness]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def init_head(head: YOLOXHead, # The YOLOX head to be initialized.\n",
    "              num_classes: int # The number of classes in the dataset.\n",
    "             ) -> None:\n",
    "    \"\"\"\n",
    "    Initialize the `YOLOXHead` with appropriate class outputs and convolution layers.\n",
    "\n",
    "    This function configures the output channels in the YOLOX head to match the\n",
    "    number of classes in the dataset. It also initializes multiple level\n",
    "    convolutional layers for each stride in the YOLOX head.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the number of output channels in the head to be equal to the number of classes\n",
    "    head.cls_out_channels = num_classes\n",
    "\n",
    "    # Create a list of 2D convolutional layers, one for each stride in the head.\n",
    "    # Each convolutional layer will have a number of output channels equal to the number of classes\n",
    "    # and a kernel size of 1 (i.e., it will perform a 1x1 convolution).\n",
    "    \n",
    "    conv_layers = [nn.Conv2d(head.feat_channels, head.cls_out_channels, 1) for _ in head.strides]\n",
    "    \n",
    "    for conv in conv_layers:\n",
    "        # Use Kaiming initialization to initialize the weights of the convolutional layers. \n",
    "        init.kaiming_normal_(conv.weight.data, mode='fan_out', nonlinearity='relu')\n",
    "    \n",
    "    head.multi_level_conv_cls = nn.ModuleList(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-2): 3 x Conv2d(96, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolox_head.multi_level_conv_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-2): 3 x Conv2d(96, 19, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_head(yolox_head, 19)\n",
    "yolox_head.multi_level_conv_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from cjm_psl_utils.core import download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_model(model_type:str, # Type of the model to be built.\n",
    "                num_classes:int, # Number of classes for the model.\n",
    "                pretrained:bool=True, # Whether to load pretrained weights.\n",
    "                checkpoint_dir:str='./pretrained_checkpoints/' # Directory to store checkpoints.\n",
    "               ) -> YOLOX: # The built YOLOX model.\n",
    "    \"\"\"\n",
    "    Builds a YOLOX model based on the given parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert model_type in MODEL_TYPES, f\"Invalid model_type. Expected one of: {MODEL_TYPES}, but got {model_type}\"\n",
    "\n",
    "    backbone_cfg = CSP_DARKNET_CFGS[model_type]\n",
    "    neck_cfg = PAFPN_CFGS[model_type]\n",
    "    head_cfg = HEAD_CFGS[model_type]\n",
    "    \n",
    "    backbone = CSPDarknet(**backbone_cfg)\n",
    "    neck = YOLOXPAFPN(**neck_cfg)\n",
    "\n",
    "    if pretrained and PRETRAINED_URLS[model_type] == None:\n",
    "        print(\"The selected model type does not have a pretrained checkpoint. Initializing model with untrained weights.\")\n",
    "        pretrained = False\n",
    "    \n",
    "    try:\n",
    "        if pretrained:\n",
    "            url = PRETRAINED_URLS[model_type]\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, Path(url).name)\n",
    "            download_file(url, checkpoint_dir)\n",
    "            \n",
    "            state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "            num_pretrained_classes = state_dict['bbox_head.multi_level_conv_cls.0.weight'].shape[0]\n",
    "            \n",
    "            head = YOLOXHead(num_classes=num_pretrained_classes, **head_cfg)\n",
    "        else:\n",
    "            head = YOLOXHead(num_classes=num_classes, **head_cfg)\n",
    "        \n",
    "        yolox = YOLOX(backbone, neck, head)\n",
    "        \n",
    "        if pretrained:\n",
    "            yolox.load_state_dict(state_dict)\n",
    "            init_head(head, num_classes)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while building the model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    return yolox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "yolox = build_model(model_type, 19, pretrained=True)\n",
    "\n",
    "test_inp = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    cls_scores, bbox_preds, objectness = yolox(test_inp)\n",
    "    \n",
    "print(f\"cls_scores: {[cls_score.shape for cls_score in cls_scores]}\")\n",
    "print(f\"bbox_preds: {[bbox_pred.shape for bbox_pred in bbox_preds]}\")\n",
    "print(f\"objectness: {[objectness.shape for objectness in objectness]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
