{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss\n",
    "\n",
    "> An implementation of the loss function for training the [YOLOX](https://arxiv.org/abs/2107.08430) object detection model based on [OpenMMLab](https://github.com/open-mmlab)’s implementation in the [mmdetection](https://github.com/open-mmlab/mmdetection) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Any, Type, List, Optional, Callable, Tuple, Union\n",
    "from functools import partial\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from cjm_yolox_pytorch.utils import multi_apply, generate_output_grids, apply_to_inputs\n",
    "from cjm_yolox_pytorch.simota import AssignResult, SimOTAAssigner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class SamplingResult:\n",
    "    \"\"\"\n",
    "    Bounding box sampling result.\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/core/bbox/samplers/sampling_result.py#L7)\n",
    "    \"\"\"\n",
    "    positive_indices: np.ndarray # Indices of the positive samples.\n",
    "    negative_indices: np.ndarray # Indices of the negative samples.\n",
    "    bboxes: np.ndarray # Array containing all bounding boxes.\n",
    "    ground_truth_bboxes: torch.Tensor # Tensor containing all ground truth bounding boxes.\n",
    "    assignment_result: AssignResult # Object that contains the ground truth indices and labels corresponding to each sample.\n",
    "    ground_truth_flags: np.ndarray # Array indicating which samples are ground truth.\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Indices of positive and negative samples\n",
    "        self.positive_bboxes = self.bboxes[self.positive_indices]\n",
    "        self.negative_bboxes = self.bboxes[self.negative_indices]\n",
    "        \n",
    "        # Bounding boxes for positive and negative samples\n",
    "        self.is_positive_ground_truth = self.ground_truth_flags[self.positive_indices]\n",
    "        self.number_of_ground_truths = self.ground_truth_bboxes.shape[0]\n",
    "        self.positive_assigned_ground_truth_indices = self.assignment_result.ground_truth_box_indices[self.positive_indices] - 1\n",
    "\n",
    "        # Check the consistency of ground truth bounding boxes and assigned indices\n",
    "        if self.ground_truth_bboxes.numel() == 0:\n",
    "            if self.positive_assigned_ground_truth_indices.numel() != 0:\n",
    "                raise ValueError('Mismatch between ground truth bounding boxes and positive assigned ground truth indices.')\n",
    "            self.positive_ground_truth_bboxes = torch.empty_like(self.ground_truth_bboxes).view(-1, 4)\n",
    "        else:\n",
    "            if len(self.ground_truth_bboxes.shape) < 2:\n",
    "                self.ground_truth_bboxes = self.ground_truth_bboxes.view(-1, 4)\n",
    "            self.positive_ground_truth_bboxes = self.ground_truth_bboxes[self.positive_assigned_ground_truth_indices, :]\n",
    "\n",
    "        # If labels are assigned, assign labels for positive samples. Otherwise, set it as None\n",
    "        if self.assignment_result.category_labels is not None:\n",
    "            self.positive_ground_truth_labels = self.assignment_result.category_labels[self.positive_indices]\n",
    "        else:\n",
    "            self.positive_ground_truth_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class YOLOXLoss:\n",
    "    \"\"\"\n",
    "    The callable YOLOXLoss class implements the loss function for training a YOLOX model.\n",
    "    \n",
    "    A YOLOXLoss instance takes the, class scores, predicted bounding boxes, objectness scores, ground truth bounding boxes, and ground truth labels. It then goes through the following steps:\n",
    "\n",
    "    1. Generate box coordinates for the output grids based on the input dimensions and stride values.\n",
    "    2. Flatten and concatenate class predictions, bounding box predictions, and objectness scores.\n",
    "    3. Decode box predictions.\n",
    "    4. Compute targets for each image in the batch.\n",
    "    5. Concatenate all positive masks, class targets, objectness targets, and bounding box targets.\n",
    "    6. Compute the bounding box loss, objectness loss, and classification loss, scale them by their respective weights, and normalize them by the total number of samples.\n",
    "    7. If using L1 loss, concatenate L1 targets, computes the L1 loss, scale it by its weight, and normalize it by the total number of samples.\n",
    "    8. Return a dictionary containing the computed losses.\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/dense_heads/yolox_head.py#L321)\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_classes:int, # The number of target classes.\n",
    "                 bbox_loss_weight:float=5.0, # The weight for the loss function to calculate the bounding box regression loss.\n",
    "                 class_loss_weight:float=1.0, # The weight for the loss function to calculate the classification loss.\n",
    "                 objectness_loss_weight:float=1.0, # The weight for the loss function to calculate the objectness loss.\n",
    "                 l1_loss_weight:float=1.0, # The weight for the loss function to calculate the L1 loss.\n",
    "                 use_l1:bool=False, # Whether to use L1 loss in the calculation.\n",
    "                 strides:List[int]=[8,16,32] # The list of strides.\n",
    "                ):\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        giou_loss_partial = partial(torchvision.ops.generalized_box_iou_loss, reduction='none', eps=1e-16)\n",
    "        self.bbox_loss_func = lambda bx1, bx2 : (1-(1-giou_loss_partial(boxes1=bx1, boxes2=bx2))**2).sum()\n",
    "        self.class_loss_func = partial(F.binary_cross_entropy_with_logits, reduction='sum')\n",
    "        self.objectness_loss_func = partial(F.binary_cross_entropy_with_logits, reduction='sum')\n",
    "        self.l1_loss_func = partial(F.l1_loss, reduction='sum')\n",
    "        \n",
    "        self.bbox_loss_weight = bbox_loss_weight\n",
    "        self.class_loss_weight = class_loss_weight\n",
    "        self.objectness_loss_weight = objectness_loss_weight\n",
    "        self.l1_loss_weight = l1_loss_weight\n",
    "        \n",
    "        self.use_l1 = use_l1\n",
    "        \n",
    "        # Initialize the assigner\n",
    "        self.assigner = SimOTAAssigner(center_radius=2.5)\n",
    "        \n",
    "        self.strides = strides\n",
    "        \n",
    "        \n",
    "    def bbox_decode(self, output_grid_boxes, predicted_boxes):\n",
    "        \"\"\"\n",
    "        Decodes the predicted bounding boxes based on the prior boxes.\n",
    "\n",
    "        Args:\n",
    "            output_grid_boxes (torch.Tensor): The output grid boxes.\n",
    "            predicted_boxes (torch.Tensor): The predicted bounding boxes.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The decoded bounding boxes.\n",
    "        \"\"\"\n",
    "        # Calculate box centroids (geometric centers) and sizes\n",
    "        box_centroids = (predicted_boxes[..., :2] * output_grid_boxes[:, 2:]) + output_grid_boxes[:, :2]\n",
    "        box_sizes = torch.exp(predicted_boxes[..., 2:]) * output_grid_boxes[:, 2:]\n",
    "\n",
    "        # Calculate corners of bounding boxes\n",
    "        top_left = box_centroids - box_sizes / 2\n",
    "        bottom_right = box_centroids + box_sizes / 2\n",
    "\n",
    "        # Stack coordinates to create decoded bounding boxes\n",
    "        decoded_boxes = torch.cat((top_left, bottom_right), -1)\n",
    "\n",
    "        return decoded_boxes\n",
    "\n",
    "\n",
    "    def sample(self, assignment_result, bboxes, ground_truth_boxes, **kwargs):\n",
    "        \"\"\"\n",
    "        Samples positive and negative indices based on the assignment result.\n",
    "        \n",
    "        Args:\n",
    "            assignment_result (Object): The assignment result obtained from assigner.\n",
    "            bboxes (torch.Tensor): The predicted bounding boxes.\n",
    "            ground_truth_boxes (torch.Tensor): The ground truth boxes.\n",
    "\n",
    "        Returns:\n",
    "            SamplingResult: The sampling result containing positive and negative indices.\n",
    "        \"\"\"\n",
    "        positive_indices = torch.nonzero(\n",
    "            assignment_result.ground_truth_box_indices > 0, as_tuple=False).squeeze(-1).unique()\n",
    "        negative_indices = torch.nonzero(\n",
    "            assignment_result.ground_truth_box_indices == 0, as_tuple=False).squeeze(-1).unique()\n",
    "        ground_truth_flags = bboxes.new_zeros(bboxes.shape[0], dtype=torch.uint8)\n",
    "        sampling_result = SamplingResult(positive_indices, negative_indices, bboxes, ground_truth_boxes,\n",
    "                                         assignment_result, ground_truth_flags)\n",
    "        return sampling_result\n",
    "\n",
    "\n",
    "    def get_l1_target(self, l1_target, ground_truth_boxes, output_grid_boxes, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Calculates the L1 target.\n",
    "\n",
    "        Args:\n",
    "            l1_target (torch.Tensor): The L1 target tensor.\n",
    "            ground_truth_boxes (torch.Tensor): The ground truth boxes.\n",
    "            output_grid_boxes (torch.Tensor): The output grid boxes.\n",
    "            epsilon (float, optional): A small value to prevent division by zero. Defaults to 1e-8.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The updated L1 target.\n",
    "        \"\"\"\n",
    "        ground_truth_centroid_and_wh = torchvision.ops.box_convert(ground_truth_boxes, 'xyxy', 'cxcywh')\n",
    "        l1_target[:, :2] = (ground_truth_centroid_and_wh[:, :2] - output_grid_boxes[:, :2]) / output_grid_boxes[:, 2:]\n",
    "        l1_target[:, 2:] = torch.log(ground_truth_centroid_and_wh[:, 2:] / output_grid_boxes[:, 2:] + epsilon)\n",
    "        return l1_target\n",
    "\n",
    "\n",
    "    def get_target_single(self, class_preds, objectness_score, output_grid_boxes, decoded_bboxes,\n",
    "                           ground_truth_bboxes, ground_truth_labels):\n",
    "        \"\"\"\n",
    "        Calculates the targets for a single image.\n",
    "\n",
    "        Args:\n",
    "            class_preds (torch.Tensor): The predicted class probabilities.\n",
    "            objectness_score (torch.Tensor): The predicted objectness scores.\n",
    "            output_grid_boxes (torch.Tensor): The output grid boxes.\n",
    "            decoded_bboxes (torch.Tensor): The decoded bounding boxes.\n",
    "            ground_truth_bboxes (torch.Tensor): The ground truth boxes.\n",
    "            ground_truth_labels (torch.Tensor): The ground truth labels.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: The targets for classification, objectness, bounding boxes, and L1 (if applicable), along with\n",
    "            the foreground mask and the number of positive samples.\n",
    "        \"\"\"\n",
    "        # Get the number of prior boxes and ground truth labels\n",
    "        num_output_grid_boxes = output_grid_boxes.size(0)\n",
    "        num_ground_truths = ground_truth_labels.size(0)\n",
    "\n",
    "        # Match dtype of ground truth bounding boxes to the dtype of decoded bounding boxes\n",
    "        ground_truth_bboxes = ground_truth_bboxes.to(decoded_bboxes.dtype)\n",
    "\n",
    "        # Check if there are no ground truth labels (objects) in the image\n",
    "        if num_ground_truths == 0:\n",
    "            # Initialize targets as zero tensors, and foreground_mask as a boolean tensor with False values\n",
    "            class_targets = class_preds.new_zeros((0, self.num_classes))\n",
    "            bbox_targets = class_preds.new_zeros((0, 4))\n",
    "            l1_targets = class_preds.new_zeros((0, 4))\n",
    "            objectness_targets = class_preds.new_zeros((num_output_grid_boxes, 1))\n",
    "            foreground_mask = class_preds.new_zeros(num_output_grid_boxes).bool()\n",
    "            return (foreground_mask, class_targets, objectness_targets, bbox_targets,\n",
    "                    l1_targets, 0)  # Return zero for num_positive_per_image\n",
    "\n",
    "        # Calculate the offset for the prior boxes\n",
    "        offset_output_grid_boxes = torch.cat([output_grid_boxes[:, :2] + output_grid_boxes[:, 2:] * 0.5, output_grid_boxes[:, 2:]], dim=-1)\n",
    "\n",
    "        # Assign ground truth objects to prior boxes and get assignment results\n",
    "        assignment_result = self.assigner.assign(\n",
    "            class_preds.sigmoid() * objectness_score.unsqueeze(1).sigmoid(),\n",
    "            offset_output_grid_boxes, decoded_bboxes, ground_truth_bboxes, ground_truth_labels)\n",
    "\n",
    "        # Use assignment results to sample prior boxes\n",
    "        sampling_result = self.sample(assignment_result, output_grid_boxes, ground_truth_bboxes)\n",
    "        \n",
    "        # Get the indices of positive (object-containing) samples\n",
    "        positive_indices = sampling_result.positive_indices\n",
    "        num_positive_per_image = positive_indices.size(0)\n",
    "\n",
    "        # Get the maximum IoU values for the positive samples\n",
    "        positive_ious = assignment_result.max_iou_values[positive_indices]\n",
    "\n",
    "        # Generate class targets\n",
    "        class_targets = F.one_hot(sampling_result.positive_ground_truth_labels, self.num_classes) * positive_ious.unsqueeze(-1)\n",
    "\n",
    "        # Initialize objectness targets as zeros and set the values at positive_indices to 1\n",
    "        objectness_targets = torch.zeros_like(objectness_score).unsqueeze(-1)\n",
    "        objectness_targets[positive_indices] = 1\n",
    "\n",
    "        # Generate bounding box targets\n",
    "        bbox_targets = sampling_result.positive_ground_truth_bboxes\n",
    "\n",
    "        # Initialize L1 targets as zeros\n",
    "        l1_targets = class_preds.new_zeros((num_positive_per_image, 4))\n",
    "\n",
    "        # If use_l1 is True, calculate L1 targets\n",
    "        if self.use_l1:\n",
    "            l1_targets = self.get_l1_target(l1_targets, bbox_targets, output_grid_boxes[positive_indices])\n",
    "\n",
    "        # Initialize foreground_mask as zeros and set the values at positive_indices to True\n",
    "        foreground_mask = torch.zeros_like(objectness_score).to(torch.bool)\n",
    "        foreground_mask[positive_indices] = 1\n",
    "\n",
    "        # Return the computed targets, the foreground mask, and the number of positive samples per image\n",
    "        return (foreground_mask, class_targets, objectness_targets, bbox_targets, l1_targets, num_positive_per_image)\n",
    "    \n",
    "    \n",
    "    def flatten_and_concat(self, tensors, batch_size, reshape_dims=None):\n",
    "        new_shape = (batch_size, -1, reshape_dims) if reshape_dims else (batch_size, -1)\n",
    "        return torch.cat([t.permute(0, 2, 3, 1).reshape(*new_shape) for t in tensors], dim=1)\n",
    "\n",
    "    \n",
    "#     def __call__(self, class_scores, predicted_bboxes, objectness_scores, ground_truth_bboxes, ground_truth_labels):\n",
    "#         \"\"\"\n",
    "#         Main method to compute the YOLOX loss.\n",
    "\n",
    "#         Args:\n",
    "#             class_scores (List[torch.Tensor]): A list of class scores for each scale.\n",
    "#             predicted_bboxes (List[torch.Tensor]): A list of predicted bounding boxes for each scale.\n",
    "#             objectness_scores (List[torch.Tensor]): A list of objectness scores for each scale.\n",
    "#             ground_truth_bboxes (List[torch.Tensor]): A list of ground truth bounding boxes for each image.\n",
    "#             ground_truth_labels (List[torch.Tensor]): A list of ground truth labels for each image.\n",
    "\n",
    "#         Returns:\n",
    "#             Dict: A dictionary with the classification, bounding box, objectness, and optionally, L1 loss.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Get the number of images in the batch\n",
    "#         batch_size = class_scores[0].shape[0]\n",
    "        \n",
    "#         # Generate box coordinates for all grid priors.\n",
    "#         output_grid_boxes = generate_output_grids(*[s*self.strides[0] for s in class_scores[0].shape[-2:]], self.strides)\n",
    "#         output_grid_boxes[:, :2] *= output_grid_boxes[:, 2].unsqueeze(1)\n",
    "#         flatten_output_grid_boxes = torch.cat([output_grid_boxes, output_grid_boxes[:, 2:].clone()], dim=1)\n",
    "        \n",
    "#         # Flatten and concatenate class predictions, bounding box predictions, and objectness scores\n",
    "#         flatten_class_preds = self.flatten_and_concat(class_scores, batch_size, self.num_classes)\n",
    "#         flatten_bbox_preds = self.flatten_and_concat(predicted_bboxes, batch_size, 4)\n",
    "#         flatten_objectness_scores = self.flatten_and_concat(objectness_scores, batch_size)\n",
    "                    \n",
    "#         # Concatenate and decode box predictions\n",
    "#         flatten_output_grid_boxes = flatten_output_grid_boxes.to(flatten_bbox_preds.device)\n",
    "#         flatten_decoded_bboxes = self.bbox_decode(flatten_output_grid_boxes, flatten_bbox_preds)\n",
    "\n",
    "#         # Compute targets\n",
    "#         (positive_masks, class_targets, objectness_targets, bbox_targets, l1_targets,\n",
    "#          num_positive_images) = multi_apply(\n",
    "#              self.get_target_single, flatten_class_preds.detach(),\n",
    "#              flatten_objectness_scores.detach(),\n",
    "#              flatten_output_grid_boxes.unsqueeze(0).repeat(batch_size, 1, 1),\n",
    "#              flatten_decoded_bboxes.detach(), ground_truth_bboxes, ground_truth_labels)\n",
    "\n",
    "#         # Concatenate all positive masks, class targets, objectness targets, and bounding box targets\n",
    "#         positive_masks = torch.cat(positive_masks, 0)\n",
    "#         class_targets = torch.cat(class_targets, 0)\n",
    "#         objectness_targets = torch.cat(objectness_targets, 0)\n",
    "#         bbox_targets = torch.cat(bbox_targets, 0)\n",
    "\n",
    "#         # Compute bounding box loss\n",
    "#         loss_bbox = self.bbox_loss_func(flatten_decoded_bboxes.view(-1, 4)[positive_masks], bbox_targets)\n",
    "\n",
    "#         # Compute objectness loss\n",
    "#         loss_obj = self.objectness_loss_func(flatten_objectness_scores.view(-1, 1), objectness_targets)\n",
    "\n",
    "#         # Compute class loss\n",
    "#         loss_cls = self.class_loss_func(flatten_class_preds.view(-1, self.num_classes)[positive_masks],class_targets)\n",
    "        \n",
    "#         # Calculate total number of samples\n",
    "#         num_total_samples = max(sum(num_positive_images), 1)\n",
    "        \n",
    "#         # Scale losses\n",
    "#         loss_bbox = (loss_bbox * self.bbox_loss_weight) / num_total_samples\n",
    "#         loss_obj = (loss_obj * self.objectness_loss_weight) / num_total_samples\n",
    "#         loss_cls = (loss_cls * self.class_loss_weight) / num_total_samples\n",
    "        \n",
    "#         # Initialize loss dictionary\n",
    "#         loss_dict = dict(loss_cls=loss_cls, loss_bbox=loss_bbox, loss_obj=loss_obj)\n",
    "\n",
    "#         # If use_l1 is True, concatenate l1 targets, compute L1 loss and add it to the loss dictionary\n",
    "#         if self.use_l1:\n",
    "#             l1_targets = torch.cat(l1_targets, 0)\n",
    "#             loss_l1 = self.l1_loss_func(\n",
    "#                 flatten_bbox_preds.view(-1, 4)[positive_masks],\n",
    "#                 l1_targets) / num_total_samples\n",
    "#             loss_l1 *= self.l1_loss_weight\n",
    "#             loss_dict.update(loss_l1=loss_l1)\n",
    "\n",
    "#         # Return loss dictionary\n",
    "#         return loss_dict\n",
    "    \n",
    "    \n",
    "    def __call__(self, class_scores, predicted_bboxes, objectness_scores, ground_truth_bboxes, ground_truth_labels):\n",
    "        \"\"\"\n",
    "        Main method to compute the YOLOX loss.\n",
    "\n",
    "        Args:\n",
    "            class_scores (List[torch.Tensor]): A list of class scores for each scale.\n",
    "            predicted_bboxes (List[torch.Tensor]): A list of predicted bounding boxes for each scale.\n",
    "            objectness_scores (List[torch.Tensor]): A list of objectness scores for each scale.\n",
    "            ground_truth_bboxes (List[torch.Tensor]): A list of ground truth bounding boxes for each image.\n",
    "            ground_truth_labels (List[torch.Tensor]): A list of ground truth labels for each image.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary with the classification, bounding box, objectness, and optionally, L1 loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the number of images in the batch\n",
    "        batch_size = class_scores[0].shape[0]\n",
    "        \n",
    "        # Generate box coordinates for all grid priors.\n",
    "        output_grid_boxes = generate_output_grids(*[s*self.strides[0] for s in class_scores[0].shape[-2:]], self.strides)\n",
    "        output_grid_boxes[:, :2] *= output_grid_boxes[:, 2].unsqueeze(1)\n",
    "        flatten_output_grid_boxes = torch.cat([output_grid_boxes, output_grid_boxes[:, 2:].clone()], dim=1)\n",
    "        \n",
    "        # Flatten and concatenate class predictions, bounding box predictions, and objectness scores\n",
    "        flatten_class_preds = self.flatten_and_concat(class_scores, batch_size, self.num_classes)\n",
    "        flatten_bbox_preds = self.flatten_and_concat(predicted_bboxes, batch_size, 4)\n",
    "        flatten_objectness_scores = self.flatten_and_concat(objectness_scores, batch_size)\n",
    "                    \n",
    "        # Concatenate and decode box predictions\n",
    "        flatten_output_grid_boxes = flatten_output_grid_boxes.to(flatten_bbox_preds.device)\n",
    "        flatten_decoded_bboxes = self.bbox_decode(flatten_output_grid_boxes, flatten_bbox_preds)\n",
    "\n",
    "        \n",
    "        # ---------------------------\n",
    "        \n",
    "        # Prepare inputs for self.get_target_single\n",
    "        inputs = zip(flatten_class_preds.detach(), flatten_objectness_scores.detach(),\n",
    "                     flatten_output_grid_boxes.unsqueeze(0).repeat(batch_size, 1, 1),\n",
    "                     flatten_decoded_bboxes.detach(), ground_truth_bboxes, ground_truth_labels)\n",
    "\n",
    "        # Apply self.get_target_single to each input, handling errors and collecting the results\n",
    "        target_results = apply_to_inputs(self.get_target_single, inputs)\n",
    "\n",
    "        # Process the target results\n",
    "        positive_masks, class_targets, objectness_targets, bbox_targets, l1_targets, num_positive_images = [], [], [], [], [], []\n",
    "        for result in target_results:\n",
    "            if result is not None:\n",
    "                positive_masks.append(result[0])\n",
    "                class_targets.append(result[1])\n",
    "                objectness_targets.append(result[2])\n",
    "                bbox_targets.append(result[3])\n",
    "                l1_targets.append(result[4])\n",
    "                num_positive_images.append(result[5])\n",
    "        \n",
    "        \n",
    "        # ---------------------------\n",
    "\n",
    "        # Concatenate all positive masks, class targets, objectness targets, and bounding box targets\n",
    "        positive_masks = torch.cat(positive_masks, 0)\n",
    "        class_targets = torch.cat(class_targets, 0)\n",
    "        objectness_targets = torch.cat(objectness_targets, 0)\n",
    "        bbox_targets = torch.cat(bbox_targets, 0)\n",
    "\n",
    "        # Compute bounding box loss\n",
    "        loss_bbox = self.bbox_loss_func(flatten_decoded_bboxes.view(-1, 4)[positive_masks], bbox_targets)\n",
    "\n",
    "        # Compute objectness loss\n",
    "        loss_obj = self.objectness_loss_func(flatten_objectness_scores.view(-1, 1), objectness_targets)\n",
    "\n",
    "        # Compute class loss\n",
    "        loss_cls = self.class_loss_func(flatten_class_preds.view(-1, self.num_classes)[positive_masks],class_targets)\n",
    "        \n",
    "        # Calculate total number of samples\n",
    "        num_total_samples = max(sum(num_positive_images), 1)\n",
    "        \n",
    "        # Scale losses\n",
    "        loss_bbox = (loss_bbox * self.bbox_loss_weight) / num_total_samples\n",
    "        loss_obj = (loss_obj * self.objectness_loss_weight) / num_total_samples\n",
    "        loss_cls = (loss_cls * self.class_loss_weight) / num_total_samples\n",
    "        \n",
    "        # Initialize loss dictionary\n",
    "        loss_dict = dict(loss_cls=loss_cls, loss_bbox=loss_bbox, loss_obj=loss_obj)\n",
    "\n",
    "        # If use_l1 is True, concatenate l1 targets, compute L1 loss and add it to the loss dictionary\n",
    "        if self.use_l1:\n",
    "            l1_targets = torch.cat(l1_targets, 0)\n",
    "            loss_l1 = self.l1_loss_func(\n",
    "                flatten_bbox_preds.view(-1, 4)[positive_masks],\n",
    "                l1_targets) / num_total_samples\n",
    "            loss_l1 *= self.l1_loss_weight\n",
    "            loss_dict.update(loss_l1=loss_l1)\n",
    "\n",
    "        # Return loss dictionary\n",
    "        return loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walkthrough\n",
    "\n",
    "The `YOLOXLoss` class implements the functionality to calculate the loss values for training a YOLOX model. This class uses multiple techniques, including bounding box decoding, positive/negative sampling, and different types of loss calculations, such as classification, objectness, and bounding box losses.\n",
    "\n",
    "### Class Initialization\n",
    "\n",
    "The `__init__` method sets up the loss function by defining several parameters:\n",
    "\n",
    "- `num_classes`: The number of target classes.\n",
    "- `bbox_loss_weight`, `class_loss_weight`, `objectness_loss_weight`, `l1_loss_weight`: These are the weights for the loss function to calculate the respective losses.\n",
    "- `use_l1`: A flag indicating whether to use L1 loss.\n",
    "- `strides`: The stride values used for traversing the model output.\n",
    "\n",
    "This method also initializes different loss functions, such as [Generalized IoU](https://pytorch.org/vision/stable/generated/torchvision.ops.generalized_box_iou_loss.html) for bounding box loss, [binary cross entropy with logits](https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy_with_logits.html) for classification and objectness loss, and [L1 loss](https://pytorch.org/docs/stable/generated/torch.nn.functional.l1_loss.html#torch.nn.functional.l1_loss) if applicable.\n",
    "\n",
    "### Bounding Box Decoding\n",
    "\n",
    "The `bbox_decode` method uses the `output_grid_boxes` to decode the predicted bounding boxes to the original image space.\n",
    "\n",
    "### Sampling\n",
    "\n",
    "The `sample` method samples positive and negative indices based on the assignment result. Positive indices are those where the ground truth box indices are greater-than zero (indicating a match with a ground truth object), and the negatives are where the ground truth box indices are zero (meaning it does not pair with a ground truth object).\n",
    "\n",
    "### L1 Target Calculation\n",
    "\n",
    "The `get_l1_target` method calculates the L1 target, which measures the absolute differences between the predicted and actual values. The L1 loss measures how well the model’s predictions match the ground truth values.\n",
    "\n",
    "### Target Calculation for Single Image\n",
    "\n",
    "The `get_target_single` method calculates the targets for a single image. It assigns ground truth objects to output grid boxes and samples output grid boxes based on the assignment results. It then generates class targets, objectness targets, bounding box targets, and, optionally, L1 targets.\n",
    "\n",
    "### Flatten and Concatenate\n",
    "\n",
    "The `flatten_and_concat` method flattens and concatenates tensors. We use this method to prepare the class predictions, bounding box predictions, and objectness scores.\n",
    "\n",
    "### Loss Calculation\n",
    "\n",
    "The `__call__` method computes the loss values. It first generates box coordinates for the output grids based on the input dimensions and stride values. It then flattens and concatenates class predictions, bounding box predictions, and objectness scores. Next, it decodes the bounding box predictions, computes targets for each image in the batch, and finally computes the bounding box loss, objectness loss, and classification loss (and L1 loss, optionally). These losses are scaled by their respective weights and normalized by the total number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
