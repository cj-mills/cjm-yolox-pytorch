{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss\n",
    "\n",
    "> An implementation of the loss function for training the [YOLOX](https://arxiv.org/abs/2107.08430) object detection model based on [OpenMMLab](https://github.com/open-mmlab)â€™s implementation in the [mmdetection](https://github.com/open-mmlab/mmdetection) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Any, Type, List, Optional, Callable, Tuple, Union, Dict\n",
    "from functools import partial\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from cjm_yolox_pytorch.utils import multi_apply, generate_output_grids\n",
    "from cjm_yolox_pytorch.simota import AssignResult, SimOTAAssigner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class SamplingResult:\n",
    "    \"\"\"\n",
    "    Bounding box sampling result.\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/core/bbox/samplers/sampling_result.py#L7)\n",
    "    \"\"\"\n",
    "    positive_indices: np.ndarray # Indices of the positive samples.\n",
    "    negative_indices: np.ndarray # Indices of the negative samples.\n",
    "    bboxes: np.ndarray # Array containing all bounding boxes.\n",
    "    ground_truth_bboxes: torch.Tensor # Tensor containing all ground truth bounding boxes.\n",
    "    assignment_result: AssignResult # Object that contains the ground truth indices and labels corresponding to each sample.\n",
    "    ground_truth_flags: np.ndarray # Array indicating which samples are ground truth.\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Indices of positive and negative samples\n",
    "        self.positive_bboxes = self.bboxes[self.positive_indices]\n",
    "        self.negative_bboxes = self.bboxes[self.negative_indices]\n",
    "        \n",
    "        # Bounding boxes for positive and negative samples\n",
    "        self.is_positive_ground_truth = self.ground_truth_flags[self.positive_indices]\n",
    "        self.number_of_ground_truths = self.ground_truth_bboxes.shape[0]\n",
    "        self.positive_assigned_ground_truth_indices = self.assignment_result.ground_truth_box_indices[self.positive_indices] - 1\n",
    "\n",
    "        # Check the consistency of ground truth bounding boxes and assigned indices\n",
    "        if self.ground_truth_bboxes.numel() == 0:\n",
    "            if self.positive_assigned_ground_truth_indices.numel() != 0:\n",
    "                raise ValueError('Mismatch between ground truth bounding boxes and positive assigned ground truth indices.')\n",
    "            self.positive_ground_truth_bboxes = torch.empty_like(self.ground_truth_bboxes).reshape(-1, 4)\n",
    "        else:\n",
    "            if len(self.ground_truth_bboxes.shape) < 2:\n",
    "                self.ground_truth_bboxes = self.ground_truth_bboxes.reshape(-1, 4)\n",
    "            self.positive_ground_truth_bboxes = self.ground_truth_bboxes[self.positive_assigned_ground_truth_indices, :]\n",
    "\n",
    "        # If labels are assigned, assign labels for positive samples. Otherwise, set it as None\n",
    "        if self.assignment_result.category_labels is not None:\n",
    "            self.positive_ground_truth_labels = self.assignment_result.category_labels[self.positive_indices]\n",
    "        else:\n",
    "            self.positive_ground_truth_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class YOLOXLoss:\n",
    "    \"\"\"\n",
    "    The callable YOLOXLoss class implements the loss function for training a YOLOX model.\n",
    "    \n",
    "    A YOLOXLoss instance takes the, class scores, predicted bounding boxes, objectness scores, ground truth bounding boxes, and ground truth labels. It then goes through the following steps:\n",
    "\n",
    "    1. Generate box coordinates for the output grids based on the input dimensions and stride values.\n",
    "    2. Flatten and concatenate class predictions, bounding box predictions, and objectness scores.\n",
    "    3. Decode box predictions.\n",
    "    4. Compute targets for each image in the batch.\n",
    "    5. Concatenate all positive masks, class targets, objectness targets, and bounding box targets.\n",
    "    6. Compute the bounding box loss, objectness loss, and classification loss, scale them by their respective weights, and normalize them by the total number of samples.\n",
    "    7. If using L1 loss, concatenate L1 targets, computes the L1 loss, scale it by its weight, and normalize it by the total number of samples.\n",
    "    8. Return a dictionary containing the computed losses.\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/dense_heads/yolox_head.py#L321)\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_classes:int, # The number of target classes.\n",
    "                 bbox_loss_weight:float=5.0, # The weight for the loss function to calculate the bounding box regression loss.\n",
    "                 class_loss_weight:float=1.0, # The weight for the loss function to calculate the classification loss.\n",
    "                 objectness_loss_weight:float=1.0, # The weight for the loss function to calculate the objectness loss.\n",
    "                 l1_loss_weight:float=1.0, # The weight for the loss function to calculate the L1 loss.\n",
    "                 use_l1:bool=False, # Whether to use L1 loss in the calculation.\n",
    "                 strides:List[int]=[8,16,32] # The list of strides.\n",
    "                ):\n",
    "        \n",
    "        \"\"\"\n",
    "        The `__init__` method defines several parameters for computing the loss, \n",
    "        and it initializes different loss functions, \n",
    "        such as [Generalized IoU](https://pytorch.org/vision/stable/generated/torchvision.ops.generalized_box_iou_loss.html) for bounding box loss, \n",
    "        [binary cross entropy with logits](https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy_with_logits.html) for classification and objectness loss, \n",
    "        and [L1 loss](https://pytorch.org/docs/stable/generated/torch.nn.functional.l1_loss.html#torch.nn.functional.l1_loss) if applicable.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        giou_loss_partial = partial(torchvision.ops.generalized_box_iou_loss, reduction='none', eps=1e-16)\n",
    "        self.bbox_loss_func = lambda bx1, bx2 : (1-(1-giou_loss_partial(boxes1=bx1, boxes2=bx2))**2).sum()\n",
    "        self.class_loss_func = partial(F.binary_cross_entropy_with_logits, reduction='sum')\n",
    "        self.objectness_loss_func = partial(F.binary_cross_entropy_with_logits, reduction='sum')\n",
    "        self.l1_loss_func = partial(F.l1_loss, reduction='sum')\n",
    "        \n",
    "        self.bbox_loss_weight = bbox_loss_weight\n",
    "        self.class_loss_weight = class_loss_weight\n",
    "        self.objectness_loss_weight = objectness_loss_weight\n",
    "        self.l1_loss_weight = l1_loss_weight\n",
    "        \n",
    "        self.use_l1 = use_l1\n",
    "        \n",
    "        # Initialize the assigner\n",
    "        self.assigner = SimOTAAssigner(center_radius=2.5)\n",
    "        \n",
    "        self.strides = strides\n",
    "        \n",
    "        \n",
    "    def bbox_decode(self, \n",
    "                    output_grid_boxes:torch.Tensor, # The output grid boxes.\n",
    "                    predicted_boxes:torch.Tensor # The predicted bounding boxes.\n",
    "                   ) -> torch.Tensor: # The decoded bounding boxes.\n",
    "        \"\"\"\n",
    "        Decodes the predicted bounding boxes based on the output grid boxes. \n",
    "        Positive indices are those where the ground truth box indices are greater-than zero (indicating a match with a ground truth object), \n",
    "        and the negatives are where the ground truth box indices are zero (meaning it does not pair with a ground truth object).\n",
    "        \"\"\"\n",
    "        # Calculate box centroids (geometric centers) and sizes\n",
    "        box_centroids = (predicted_boxes[..., :2] * output_grid_boxes[:, 2:]) + output_grid_boxes[:, :2]\n",
    "        box_sizes = torch.exp(predicted_boxes[..., 2:]) * output_grid_boxes[:, 2:]\n",
    "\n",
    "        # Calculate corners of bounding boxes\n",
    "        top_left = box_centroids - box_sizes / 2\n",
    "        bottom_right = box_centroids + box_sizes / 2\n",
    "\n",
    "        # Stack coordinates to create decoded bounding boxes\n",
    "        decoded_boxes = torch.cat((top_left, bottom_right), -1)\n",
    "\n",
    "        return decoded_boxes\n",
    "\n",
    "\n",
    "    def sample(self, \n",
    "               assignment_result:AssignResult, # The assignment result obtained from assigner.\n",
    "               bboxes:torch.Tensor, #  The predicted bounding boxes.\n",
    "               ground_truth_boxes:torch.Tensor, # The ground truth boxes.\n",
    "              ) -> SamplingResult: # The sampling result containing positive and negative indices.\n",
    "        \"\"\"\n",
    "        Samples positive and negative indices based on the assignment result.\n",
    "        \"\"\"\n",
    "        positive_indices = torch.nonzero(\n",
    "            assignment_result.ground_truth_box_indices > 0, as_tuple=False).squeeze(-1).unique()\n",
    "        negative_indices = torch.nonzero(\n",
    "            assignment_result.ground_truth_box_indices == 0, as_tuple=False).squeeze(-1).unique()\n",
    "        ground_truth_flags = bboxes.new_zeros(bboxes.shape[0], dtype=torch.uint8)\n",
    "        sampling_result = SamplingResult(positive_indices, negative_indices, bboxes, ground_truth_boxes,\n",
    "                                         assignment_result, ground_truth_flags)\n",
    "        return sampling_result\n",
    "\n",
    "\n",
    "    def get_l1_target(self, \n",
    "                      l1_target:torch.Tensor, # The L1 target tensor.\n",
    "                      ground_truth_boxes:torch.Tensor, # The ground truth boxes.\n",
    "                      output_grid_boxes:torch.Tensor, # The output grid boxes.\n",
    "                      epsilon:float=1e-8 # A small value to prevent division by zero.\n",
    "                     ) -> torch.Tensor: # The updated L1 target.\n",
    "        \"\"\"\n",
    "        Calculates the L1 target, which measures the absolute differences between the predicted and actual values. \n",
    "        The L1 loss measures how well the modelâ€™s predictions match the ground truth values.\n",
    "        \"\"\"\n",
    "        ground_truth_centroid_and_wh = torchvision.ops.box_convert(ground_truth_boxes, 'xyxy', 'cxcywh')\n",
    "        l1_target[:, :2] = (ground_truth_centroid_and_wh[:, :2] - output_grid_boxes[:, :2]) / output_grid_boxes[:, 2:]\n",
    "        l1_target[:, 2:] = torch.log(ground_truth_centroid_and_wh[:, 2:] / output_grid_boxes[:, 2:] + epsilon)\n",
    "        return l1_target\n",
    "\n",
    "\n",
    "    def get_target_single(self, \n",
    "                          class_preds:torch.Tensor, # The predicted class probabilities.\n",
    "                          objectness_score:torch.Tensor, # The predicted objectness scores.\n",
    "                          output_grid_boxes:torch.Tensor, # The output grid boxes.\n",
    "                          decoded_bboxes:torch.Tensor, # The decoded bounding boxes.\n",
    "                          ground_truth_bboxes:torch.Tensor, # The ground truth boxes.\n",
    "                          ground_truth_labels:torch.Tensor # The ground truth labels.\n",
    "                         ) -> Tuple: # The targets for classification, objectness, bounding boxes, and L1 (if applicable), along with the foreground mask and the number of positive samples.\n",
    "        \"\"\"\n",
    "        Calculates the targets for a single image. \n",
    "        It assigns ground truth objects to output grid boxes and samples output grid boxes based on the assignment results. \n",
    "        It then generates class targets, objectness targets, bounding box targets, and, optionally, L1 targets.\n",
    "        \"\"\"\n",
    "        # Get the number of prior boxes and ground truth labels\n",
    "        num_output_grid_boxes = output_grid_boxes.size(0)\n",
    "        num_ground_truths = ground_truth_labels.size(0)\n",
    "\n",
    "        # Match dtype of ground truth bounding boxes to the dtype of decoded bounding boxes\n",
    "        ground_truth_bboxes = ground_truth_bboxes.to(decoded_bboxes.dtype)\n",
    "\n",
    "        # Check if there are no ground truth labels (objects) in the image\n",
    "        if num_ground_truths == 0:\n",
    "            # Initialize targets as zero tensors, and foreground_mask as a boolean tensor with False values\n",
    "            class_targets = class_preds.new_zeros((0, self.num_classes))\n",
    "            bbox_targets = class_preds.new_zeros((0, 4))\n",
    "            l1_targets = class_preds.new_zeros((0, 4))\n",
    "            objectness_targets = class_preds.new_zeros((num_output_grid_boxes, 1))\n",
    "            foreground_mask = class_preds.new_zeros(num_output_grid_boxes).bool()\n",
    "            return (foreground_mask, class_targets, objectness_targets, bbox_targets,\n",
    "                    l1_targets, 0)  # Return zero for num_positive_per_image\n",
    "\n",
    "        # Calculate the offset for the prior boxes\n",
    "        offset_output_grid_boxes = torch.cat([output_grid_boxes[:, :2] + output_grid_boxes[:, 2:] * 0.5, output_grid_boxes[:, 2:]], dim=-1)\n",
    "\n",
    "        \n",
    "        # Assign ground truth objects to prior boxes and get assignment results\n",
    "        assignment_result = self.assigner.assign(\n",
    "            class_preds.sigmoid() * objectness_score.unsqueeze(1).sigmoid(),\n",
    "            offset_output_grid_boxes, decoded_bboxes, ground_truth_bboxes, ground_truth_labels)\n",
    "        \n",
    "        # Use assignment results to sample prior boxes\n",
    "        sampling_result = self.sample(assignment_result, output_grid_boxes, ground_truth_bboxes)\n",
    "        \n",
    "        # Get the indices of positive (object-containing) samples\n",
    "        positive_indices = sampling_result.positive_indices\n",
    "        num_positive_per_image = positive_indices.size(0)\n",
    "\n",
    "        # Get the maximum IoU values for the positive samples\n",
    "        positive_ious = assignment_result.max_iou_values[positive_indices]\n",
    "\n",
    "        # Generate class targets\n",
    "        class_targets = F.one_hot(sampling_result.positive_ground_truth_labels, self.num_classes) * positive_ious.unsqueeze(-1)\n",
    "\n",
    "        # Initialize objectness targets as zeros and set the values at positive_indices to 1\n",
    "        objectness_targets = torch.zeros_like(objectness_score).unsqueeze(-1)\n",
    "        objectness_targets[positive_indices] = 1\n",
    "\n",
    "        # Generate bounding box targets\n",
    "        bbox_targets = sampling_result.positive_ground_truth_bboxes\n",
    "\n",
    "        # Initialize L1 targets as zeros\n",
    "        l1_targets = class_preds.new_zeros((num_positive_per_image, 4))\n",
    "\n",
    "        # If use_l1 is True, calculate L1 targets\n",
    "        if self.use_l1:\n",
    "            l1_targets = self.get_l1_target(l1_targets, bbox_targets, output_grid_boxes[positive_indices])\n",
    "\n",
    "        # Initialize foreground_mask as zeros and set the values at positive_indices to True\n",
    "        foreground_mask = torch.zeros_like(objectness_score).to(torch.bool)\n",
    "        foreground_mask[positive_indices] = 1\n",
    "\n",
    "        # Return the computed targets, the foreground mask, and the number of positive samples per image\n",
    "        return (foreground_mask, class_targets, objectness_targets, bbox_targets, l1_targets, num_positive_per_image)\n",
    "    \n",
    "    \n",
    "    def flatten_and_concat(self, \n",
    "                           tensors:List[torch.Tensor], # A list of tensors to flatten and concatenate.\n",
    "                           batch_size:int, # The batch size used to reshape the concatenated tensor.\n",
    "                           reshape_dims:Optional[bool]=None # \n",
    "                          ) -> torch.Tensor: # The concatenated tensor\n",
    "        \"\"\"\n",
    "        Flatten and concatenate a list of tensors.\n",
    "        \"\"\"\n",
    "        new_shape = (batch_size, -1, reshape_dims) if reshape_dims else (batch_size, -1)\n",
    "        return torch.cat([t.permute(0, 2, 3, 1).reshape(*new_shape) for t in tensors], dim=1)\n",
    "\n",
    "    \n",
    "    def __call__(self, \n",
    "                 class_scores:List[torch.Tensor], # A list of class scores for each scale.\n",
    "                 predicted_bboxes:List[torch.Tensor], # A list of predicted bounding boxes for each scale.\n",
    "                 objectness_scores:List[torch.Tensor], # A list of objectness scores for each scale.\n",
    "                 ground_truth_bboxes:List[torch.Tensor], # A list of ground truth bounding boxes for each image.\n",
    "                 ground_truth_labels:List[torch.Tensor] # A list of ground truth labels for each image.\n",
    "                ) -> Dict: # A dictionary with the classification, bounding box, objectness, and optionally, L1 loss.\n",
    "        \"\"\"\n",
    "        The `__call__` method computes the loss values. \n",
    "        It first generates box coordinates for the output grids based on the input dimensions and stride values. \n",
    "        It then flattens and concatenates class predictions, bounding box predictions, and objectness scores. \n",
    "        Next, it decodes the bounding box predictions, computes targets for each image in the batch, \n",
    "        and finally computes the bounding box loss, objectness loss, and classification loss (and L1 loss, optionally). \n",
    "        These losses are scaled by their respective weights and normalized by the total number of samples.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the number of images in the batch\n",
    "        batch_size = class_scores[0].shape[0]\n",
    "        \n",
    "        # Generate box coordinates for all grid priors.\n",
    "        output_grid_boxes = generate_output_grids(*[s*self.strides[0] for s in class_scores[0].shape[-2:]], self.strides)\n",
    "        output_grid_boxes[:, :2] *= output_grid_boxes[:, 2].unsqueeze(1)\n",
    "        flatten_output_grid_boxes = torch.cat([output_grid_boxes, output_grid_boxes[:, 2:].clone()], dim=1)\n",
    "        \n",
    "        # Flatten and concatenate class predictions, bounding box predictions, and objectness scores\n",
    "        flatten_class_preds = self.flatten_and_concat(class_scores, batch_size, self.num_classes)\n",
    "        flatten_bbox_preds = self.flatten_and_concat(predicted_bboxes, batch_size, 4)\n",
    "        flatten_objectness_scores = self.flatten_and_concat(objectness_scores, batch_size)\n",
    "                    \n",
    "        # Concatenate and decode box predictions\n",
    "        flatten_output_grid_boxes = flatten_output_grid_boxes.to(flatten_bbox_preds.device)\n",
    "        flatten_decoded_bboxes = self.bbox_decode(flatten_output_grid_boxes, flatten_bbox_preds)\n",
    "\n",
    "        # Compute targets\n",
    "        (positive_masks, class_targets, objectness_targets, bbox_targets, l1_targets,\n",
    "         num_positive_images) = multi_apply(\n",
    "             self.get_target_single, flatten_class_preds.detach(),\n",
    "             flatten_objectness_scores.detach(),\n",
    "             flatten_output_grid_boxes.unsqueeze(0).repeat(batch_size, 1, 1),\n",
    "             flatten_decoded_bboxes.detach(), ground_truth_bboxes, ground_truth_labels)\n",
    "\n",
    "        # Concatenate all positive masks, class targets, objectness targets, and bounding box targets\n",
    "        positive_masks = torch.cat(positive_masks, 0)\n",
    "        class_targets = torch.cat(class_targets, 0)\n",
    "        objectness_targets = torch.cat(objectness_targets, 0)\n",
    "        bbox_targets = torch.cat(bbox_targets, 0)\n",
    "\n",
    "        # Compute bounding box loss\n",
    "        loss_bbox = self.bbox_loss_func(flatten_decoded_bboxes.reshape(-1, 4)[positive_masks], bbox_targets)\n",
    "\n",
    "        # Compute objectness loss\n",
    "        loss_obj = self.objectness_loss_func(flatten_objectness_scores.reshape(-1, 1), objectness_targets)\n",
    "\n",
    "        # Compute class loss\n",
    "        loss_cls = self.class_loss_func(flatten_class_preds.reshape(-1, self.num_classes)[positive_masks],class_targets)\n",
    "        \n",
    "        # Calculate total number of samples\n",
    "        num_total_samples = max(sum(num_positive_images), 1)\n",
    "        \n",
    "        # Scale losses\n",
    "        loss_bbox = (loss_bbox * self.bbox_loss_weight) / num_total_samples\n",
    "        loss_obj = (loss_obj * self.objectness_loss_weight) / num_total_samples\n",
    "        loss_cls = (loss_cls * self.class_loss_weight) / num_total_samples\n",
    "        \n",
    "        # Initialize loss dictionary\n",
    "        loss_dict = dict(loss_cls=loss_cls, loss_bbox=loss_bbox, loss_obj=loss_obj)\n",
    "\n",
    "        # If use_l1 is True, concatenate l1 targets, compute L1 loss and add it to the loss dictionary\n",
    "        if self.use_l1:\n",
    "            l1_targets = torch.cat(l1_targets, 0)\n",
    "            loss_l1 = self.l1_loss_func(\n",
    "                flatten_bbox_preds.reshape(-1, 4)[positive_masks],\n",
    "                l1_targets) / num_total_samples\n",
    "            loss_l1 *= self.l1_loss_weight\n",
    "            loss_dict.update(loss_l1=loss_l1)\n",
    "\n",
    "        # Return loss dictionary\n",
    "        return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/cj-mills/cjm-yolox-pytorch/blob/main/cjm_yolox_pytorch/loss.py#L88){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### YOLOXLoss.__init__\n",
       "\n",
       ">      YOLOXLoss.__init__ (num_classes:int, bbox_loss_weight:float=5.0,\n",
       ">                          class_loss_weight:float=1.0,\n",
       ">                          objectness_loss_weight:float=1.0,\n",
       ">                          l1_loss_weight:float=1.0, use_l1:bool=False,\n",
       ">                          strides:List[int]=[8, 16, 32])\n",
       "\n",
       "*The `__init__` method defines several parameters for computing the loss, \n",
       "and it initializes different loss functions, \n",
       "such as [Generalized IoU](https://pytorch.org/vision/stable/generated/torchvision.ops.generalized_box_iou_loss.html) for bounding box loss, \n",
       "[binary cross entropy with logits](https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy_with_logits.html) for classification and objectness loss, \n",
       "and [L1 loss](https://pytorch.org/docs/stable/generated/torch.nn.functional.l1_loss.html#torch.nn.functional.l1_loss) if applicable.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| num_classes | int |  | The number of target classes. |\n",
       "| bbox_loss_weight | float | 5.0 | The weight for the loss function to calculate the bounding box regression loss. |\n",
       "| class_loss_weight | float | 1.0 | The weight for the loss function to calculate the classification loss. |\n",
       "| objectness_loss_weight | float | 1.0 | The weight for the loss function to calculate the objectness loss. |\n",
       "| l1_loss_weight | float | 1.0 | The weight for the loss function to calculate the L1 loss. |\n",
       "| use_l1 | bool | False | Whether to use L1 loss in the calculation. |\n",
       "| strides | List | [8, 16, 32] | The list of strides. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/cj-mills/cjm-yolox-pytorch/blob/main/cjm_yolox_pytorch/loss.py#L88){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### YOLOXLoss.__init__\n",
       "\n",
       ">      YOLOXLoss.__init__ (num_classes:int, bbox_loss_weight:float=5.0,\n",
       ">                          class_loss_weight:float=1.0,\n",
       ">                          objectness_loss_weight:float=1.0,\n",
       ">                          l1_loss_weight:float=1.0, use_l1:bool=False,\n",
       ">                          strides:List[int]=[8, 16, 32])\n",
       "\n",
       "*The `__init__` method defines several parameters for computing the loss, \n",
       "and it initializes different loss functions, \n",
       "such as [Generalized IoU](https://pytorch.org/vision/stable/generated/torchvision.ops.generalized_box_iou_loss.html) for bounding box loss, \n",
       "[binary cross entropy with logits](https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy_with_logits.html) for classification and objectness loss, \n",
       "and [L1 loss](https://pytorch.org/docs/stable/generated/torch.nn.functional.l1_loss.html#torch.nn.functional.l1_loss) if applicable.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| num_classes | int |  | The number of target classes. |\n",
       "| bbox_loss_weight | float | 5.0 | The weight for the loss function to calculate the bounding box regression loss. |\n",
       "| class_loss_weight | float | 1.0 | The weight for the loss function to calculate the classification loss. |\n",
       "| objectness_loss_weight | float | 1.0 | The weight for the loss function to calculate the objectness loss. |\n",
       "| l1_loss_weight | float | 1.0 | The weight for the loss function to calculate the L1 loss. |\n",
       "| use_l1 | bool | False | Whether to use L1 loss in the calculation. |\n",
       "| strides | List | [8, 16, 32] | The list of strides. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(YOLOXLoss.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/cj-mills/cjm-yolox-pytorch/blob/main/cjm_yolox_pytorch/loss.py#L127){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### YOLOXLoss.bbox_decode\n",
       "\n",
       ">      YOLOXLoss.bbox_decode (output_grid_boxes:torch.Tensor,\n",
       ">                             predicted_boxes:torch.Tensor)\n",
       "\n",
       "*Decodes the predicted bounding boxes based on the output grid boxes. \n",
       "Positive indices are those where the ground truth box indices are greater-than zero (indicating a match with a ground truth object), \n",
       "and the negatives are where the ground truth box indices are zero (meaning it does not pair with a ground truth object).*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| output_grid_boxes | Tensor | The output grid boxes. |\n",
       "| predicted_boxes | Tensor | The predicted bounding boxes. |\n",
       "| **Returns** | **Tensor** | **The decoded bounding boxes.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/cj-mills/cjm-yolox-pytorch/blob/main/cjm_yolox_pytorch/loss.py#L127){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### YOLOXLoss.bbox_decode\n",
       "\n",
       ">      YOLOXLoss.bbox_decode (output_grid_boxes:torch.Tensor,\n",
       ">                             predicted_boxes:torch.Tensor)\n",
       "\n",
       "*Decodes the predicted bounding boxes based on the output grid boxes. \n",
       "Positive indices are those where the ground truth box indices are greater-than zero (indicating a match with a ground truth object), \n",
       "and the negatives are where the ground truth box indices are zero (meaning it does not pair with a ground truth object).*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| output_grid_boxes | Tensor | The output grid boxes. |\n",
       "| predicted_boxes | Tensor | The predicted bounding boxes. |\n",
       "| **Returns** | **Tensor** | **The decoded bounding boxes.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(YOLOXLoss.bbox_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/cj-mills/cjm-yolox-pytorch/blob/main/cjm_yolox_pytorch/loss.py#L150){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### YOLOXLoss.sample\n",
       "\n",
       ">      YOLOXLoss.sample\n",
       ">                        (assignment_result:cjm_yolox_pytorch.simota.AssignResul\n",
       ">                        t, bboxes:torch.Tensor,\n",
       ">                        ground_truth_boxes:torch.Tensor)\n",
       "\n",
       "*Samples positive and negative indices based on the assignment result.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| assignment_result | AssignResult | The assignment result obtained from assigner. |\n",
       "| bboxes | Tensor | The predicted bounding boxes. |\n",
       "| ground_truth_boxes | Tensor | The ground truth boxes. |\n",
       "| **Returns** | **SamplingResult** | **The sampling result containing positive and negative indices.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/cj-mills/cjm-yolox-pytorch/blob/main/cjm_yolox_pytorch/loss.py#L150){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### YOLOXLoss.sample\n",
       "\n",
       ">      YOLOXLoss.sample\n",
       ">                        (assignment_result:cjm_yolox_pytorch.simota.AssignResul\n",
       ">                        t, bboxes:torch.Tensor,\n",
       ">                        ground_truth_boxes:torch.Tensor)\n",
       "\n",
       "*Samples positive and negative indices based on the assignment result.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| assignment_result | AssignResult | The assignment result obtained from assigner. |\n",
       "| bboxes | Tensor | The predicted bounding boxes. |\n",
       "| ground_truth_boxes | Tensor | The ground truth boxes. |\n",
       "| **Returns** | **SamplingResult** | **The sampling result containing positive and negative indices.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(YOLOXLoss.sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/cj-mills/cjm-yolox-pytorch/blob/main/cjm_yolox_pytorch/loss.py#L168){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### YOLOXLoss.get_l1_target\n",
       "\n",
       ">      YOLOXLoss.get_l1_target (l1_target:torch.Tensor,\n",
       ">                               ground_truth_boxes:torch.Tensor,\n",
       ">                               output_grid_boxes:torch.Tensor,\n",
       ">                               epsilon:float=1e-08)\n",
       "\n",
       "*Calculates the L1 target, which measures the absolute differences between the predicted and actual values. \n",
       "The L1 loss measures how well the modelâ€™s predictions match the ground truth values.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| l1_target | Tensor |  | The L1 target tensor. |\n",
       "| ground_truth_boxes | Tensor |  | The ground truth boxes. |\n",
       "| output_grid_boxes | Tensor |  | The output grid boxes. |\n",
       "| epsilon | float | 1e-08 | A small value to prevent division by zero. |\n",
       "| **Returns** | **Tensor** |  | **The updated L1 target.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/cj-mills/cjm-yolox-pytorch/blob/main/cjm_yolox_pytorch/loss.py#L168){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### YOLOXLoss.get_l1_target\n",
       "\n",
       ">      YOLOXLoss.get_l1_target (l1_target:torch.Tensor,\n",
       ">                               ground_truth_boxes:torch.Tensor,\n",
       ">                               output_grid_boxes:torch.Tensor,\n",
       ">                               epsilon:float=1e-08)\n",
       "\n",
       "*Calculates the L1 target, which measures the absolute differences between the predicted and actual values. \n",
       "The L1 loss measures how well the modelâ€™s predictions match the ground truth values.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| l1_target | Tensor |  | The L1 target tensor. |\n",
       "| ground_truth_boxes | Tensor |  | The ground truth boxes. |\n",
       "| output_grid_boxes | Tensor |  | The output grid boxes. |\n",
       "| epsilon | float | 1e-08 | A small value to prevent division by zero. |\n",
       "| **Returns** | **Tensor** |  | **The updated L1 target.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(YOLOXLoss.get_l1_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/cj-mills/cjm-yolox-pytorch/blob/main/cjm_yolox_pytorch/loss.py#L184){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### YOLOXLoss.get_target_single\n",
       "\n",
       ">      YOLOXLoss.get_target_single (class_preds:torch.Tensor,\n",
       ">                                   objectness_score:torch.Tensor,\n",
       ">                                   output_grid_boxes:torch.Tensor,\n",
       ">                                   decoded_bboxes:torch.Tensor,\n",
       ">                                   ground_truth_bboxes:torch.Tensor,\n",
       ">                                   ground_truth_labels:torch.Tensor)\n",
       "\n",
       "*Calculates the targets for a single image. \n",
       "It assigns ground truth objects to output grid boxes and samples output grid boxes based on the assignment results. \n",
       "It then generates class targets, objectness targets, bounding box targets, and, optionally, L1 targets.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| class_preds | Tensor | The predicted class probabilities. |\n",
       "| objectness_score | Tensor | The predicted objectness scores. |\n",
       "| output_grid_boxes | Tensor | The output grid boxes. |\n",
       "| decoded_bboxes | Tensor | The decoded bounding boxes. |\n",
       "| ground_truth_bboxes | Tensor | The ground truth boxes. |\n",
       "| ground_truth_labels | Tensor | The ground truth labels. |\n",
       "| **Returns** | **Tuple** | **The targets for classification, objectness, bounding boxes, and L1 (if applicable), along with the foreground mask and the number of positive samples.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/cj-mills/cjm-yolox-pytorch/blob/main/cjm_yolox_pytorch/loss.py#L184){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### YOLOXLoss.get_target_single\n",
       "\n",
       ">      YOLOXLoss.get_target_single (class_preds:torch.Tensor,\n",
       ">                                   objectness_score:torch.Tensor,\n",
       ">                                   output_grid_boxes:torch.Tensor,\n",
       ">                                   decoded_bboxes:torch.Tensor,\n",
       ">                                   ground_truth_bboxes:torch.Tensor,\n",
       ">                                   ground_truth_labels:torch.Tensor)\n",
       "\n",
       "*Calculates the targets for a single image. \n",
       "It assigns ground truth objects to output grid boxes and samples output grid boxes based on the assignment results. \n",
       "It then generates class targets, objectness targets, bounding box targets, and, optionally, L1 targets.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| class_preds | Tensor | The predicted class probabilities. |\n",
       "| objectness_score | Tensor | The predicted objectness scores. |\n",
       "| output_grid_boxes | Tensor | The output grid boxes. |\n",
       "| decoded_bboxes | Tensor | The decoded bounding boxes. |\n",
       "| ground_truth_bboxes | Tensor | The ground truth boxes. |\n",
       "| ground_truth_labels | Tensor | The ground truth labels. |\n",
       "| **Returns** | **Tuple** | **The targets for classification, objectness, bounding boxes, and L1 (if applicable), along with the foreground mask and the number of positive samples.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(YOLOXLoss.get_target_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/cj-mills/cjm-yolox-pytorch/blob/main/cjm_yolox_pytorch/loss.py#L259){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### YOLOXLoss.flatten_and_concat\n",
       "\n",
       ">      YOLOXLoss.flatten_and_concat (tensors:List[torch.Tensor], batch_size:int,\n",
       ">                                    reshape_dims:Optional[bool]=None)\n",
       "\n",
       "*Flatten and concatenate a list of tensors.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tensors | List |  | A list of tensors to flatten and concatenate. |\n",
       "| batch_size | int |  | The batch size used to reshape the concatenated tensor. |\n",
       "| reshape_dims | Optional | None |  |\n",
       "| **Returns** | **Tensor** |  | **The concatenated tensor** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/cj-mills/cjm-yolox-pytorch/blob/main/cjm_yolox_pytorch/loss.py#L259){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### YOLOXLoss.flatten_and_concat\n",
       "\n",
       ">      YOLOXLoss.flatten_and_concat (tensors:List[torch.Tensor], batch_size:int,\n",
       ">                                    reshape_dims:Optional[bool]=None)\n",
       "\n",
       "*Flatten and concatenate a list of tensors.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tensors | List |  | A list of tensors to flatten and concatenate. |\n",
       "| batch_size | int |  | The batch size used to reshape the concatenated tensor. |\n",
       "| reshape_dims | Optional | None |  |\n",
       "| **Returns** | **Tensor** |  | **The concatenated tensor** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(YOLOXLoss.flatten_and_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/cj-mills/cjm-yolox-pytorch/blob/main/cjm_yolox_pytorch/loss.py#L271){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### YOLOXLoss.__call__\n",
       "\n",
       ">      YOLOXLoss.__call__ (class_scores:List[torch.Tensor],\n",
       ">                          predicted_bboxes:List[torch.Tensor],\n",
       ">                          objectness_scores:List[torch.Tensor],\n",
       ">                          ground_truth_bboxes:List[torch.Tensor],\n",
       ">                          ground_truth_labels:List[torch.Tensor])\n",
       "\n",
       "*The `__call__` method computes the loss values. \n",
       "It first generates box coordinates for the output grids based on the input dimensions and stride values. \n",
       "It then flattens and concatenates class predictions, bounding box predictions, and objectness scores. \n",
       "Next, it decodes the bounding box predictions, computes targets for each image in the batch, \n",
       "and finally computes the bounding box loss, objectness loss, and classification loss (and L1 loss, optionally). \n",
       "These losses are scaled by their respective weights and normalized by the total number of samples.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| class_scores | List | A list of class scores for each scale. |\n",
       "| predicted_bboxes | List | A list of predicted bounding boxes for each scale. |\n",
       "| objectness_scores | List | A list of objectness scores for each scale. |\n",
       "| ground_truth_bboxes | List | A list of ground truth bounding boxes for each image. |\n",
       "| ground_truth_labels | List | A list of ground truth labels for each image. |\n",
       "| **Returns** | **Dict** | **A dictionary with the classification, bounding box, objectness, and optionally, L1 loss.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/cj-mills/cjm-yolox-pytorch/blob/main/cjm_yolox_pytorch/loss.py#L271){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### YOLOXLoss.__call__\n",
       "\n",
       ">      YOLOXLoss.__call__ (class_scores:List[torch.Tensor],\n",
       ">                          predicted_bboxes:List[torch.Tensor],\n",
       ">                          objectness_scores:List[torch.Tensor],\n",
       ">                          ground_truth_bboxes:List[torch.Tensor],\n",
       ">                          ground_truth_labels:List[torch.Tensor])\n",
       "\n",
       "*The `__call__` method computes the loss values. \n",
       "It first generates box coordinates for the output grids based on the input dimensions and stride values. \n",
       "It then flattens and concatenates class predictions, bounding box predictions, and objectness scores. \n",
       "Next, it decodes the bounding box predictions, computes targets for each image in the batch, \n",
       "and finally computes the bounding box loss, objectness loss, and classification loss (and L1 loss, optionally). \n",
       "These losses are scaled by their respective weights and normalized by the total number of samples.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| class_scores | List | A list of class scores for each scale. |\n",
       "| predicted_bboxes | List | A list of predicted bounding boxes for each scale. |\n",
       "| objectness_scores | List | A list of objectness scores for each scale. |\n",
       "| ground_truth_bboxes | List | A list of ground truth bounding boxes for each image. |\n",
       "| ground_truth_labels | List | A list of ground truth labels for each image. |\n",
       "| **Returns** | **Dict** | **A dictionary with the classification, bounding box, objectness, and optionally, L1 loss.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(YOLOXLoss.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
